---
IR Note

---

> è¯¾ç¨‹ç¼–ç ï¼šCOMP3009J-Information Retrieval
>
> è€ƒæ ¸å½¢å¼ï¼š
>
> æˆè¯¾æ•™å¸ˆï¼š

# 00b-What is IR?

## 1. ä»€ä¹ˆæ˜¯ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ(Information Retrieval, IR)

ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä¸»è¦è§£å†³ç”¨æˆ·é€šè¿‡æŸ¥è¯¢ï¼Œä»å¤§é‡ä¿¡æ¯ä¸­è·å–å…¶æ„Ÿå…´è¶£å†…å®¹çš„é—®é¢˜ã€‚å…¸å‹ä¾‹å­æ˜¯æœç´¢å¼•æ“ï¼ˆå¦‚Googleã€ç™¾åº¦ï¼‰ï¼Œå®ƒä»¬å¯ä»¥ä»æ•°åäº¿ç½‘é¡µä¸­ç­›é€‰å‡ºæœ€ç›¸å…³çš„å†…å®¹å‘ˆç°ç»™ç”¨æˆ·ã€‚

**å¸¸è§IR**

- Web Search Engines
- Desktop Search
- Mobile Search
- Library Search
- Searching Individual Web Sites

## 2. Information Items

**Document**

å¯ä»¥æ˜¯ï¼šBooks, video, images, etc.

## 3. Structured vs Unstructured Data

- **ç»“æ„åŒ–æ•°æ®**ï¼šå¦‚æ•°æ®åº“æˆ–è¡¨æ ¼ï¼Œæ”¯æŒæ•°å€¼èŒƒå›´ã€ç²¾ç¡®åŒ¹é…ç­‰æŸ¥è¯¢ã€‚tables
- **éç»“æ„åŒ–æ•°æ®**ï¼šè‡ªç”±æ–‡æœ¬ï¼ˆå¦‚ç½‘é¡µã€æ–‡ç« ï¼‰ï¼Œéœ€ç”¨å…³é”®è¯æˆ–æ¦‚å¿µæŸ¥è¯¢ã€‚free-form text written in natural language

> ç°å®ä¸­ï¼Œå¤§å¤šæ•°æ–‡æ¡£ä¸º**åŠç»“æ„åŒ–æ•°æ®**ï¼Œå¦‚ç½‘é¡µåŒ…å«æ ‡é¢˜ã€æ­£æ–‡ã€é“¾æ¥ç­‰éƒ¨åˆ†ã€‚

## 4. Representation

å°†æ–‡æ¡£ä»¥æŸç§æ•°å­¦å½¢å¼è¡¨ç¤ºä»¥æ”¯æŒæ£€ç´¢ï¼ŒåŸå› ï¼š

- æ–‡æœ¬åŒ¹é…æ•ˆç‡ä½
- è®¡ç®—æœºæ“…é•¿æ•°å­¦è®¡ç®—

## 5. Storage, Organisation, Acceess

**å­˜å‚¨ä¸ç»„ç»‡**ï¼šåˆç†ç»„ç»‡æ–‡æ¡£ä¾¿äºé«˜æ•ˆæ£€ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ–‡æ¡£é›†ä¸‹ã€‚

**è®¿é—®**ï¼šIRç³»ç»Ÿçš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©ç”¨æˆ·èƒ½åŠæ—¶ã€æœ‰æ•ˆåœ°è®¿é—®æ‰€éœ€ä¿¡æ¯ã€‚

## 6. Information Need

User only use IR when there is some information that they are interested in reading.(ç”¨æˆ·ä»…åœ¨æœ‰ä¿¡æ¯éœ€æ±‚æ—¶ä½¿ç”¨IR)

**Information Need 4 Stages:** (2024å¹´è€ƒå¯Ÿ, 2023å¹´è€ƒå¯Ÿ)

- Visceral Need: æœªè¡¨è¾¾çš„æ¨¡ç³Šéœ€æ±‚	ï¼ˆactual, but unexpressed)
- Conscious Need: å¤´è„‘ä¸­æ„è¯†åˆ°çš„éœ€æ±‚ï¼Œéš¾ä»¥æ¸…æ™°è¡¨è¾¾    
- Formalised Need: å¯ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„æ­£å¼éœ€æ±‚
- Compromised Need: å®é™…æäº¤ç»™IRç³»ç»Ÿçš„æŸ¥è¯¢(query)

## 7. æŸ¥è¯¢çš„è¡¨è¾¾æ–¹å¼

- **Keyword-Based Querying:** ä½¿ç”¨2-3ä¸ªå…³é”®è¯æŸ¥è¯¢
- **Context Queries:** å…³æ³¨å…³é”®è¯åœ¨æ–‡æ¡£ä¸­çš„ç›¸å¯¹ä½ç½®
- **Boolean Queries:** ä½¿ç”¨ANDã€ORã€NOTè¿æ¥å…³é”®è¯
- **è‡ªç„¶è¯­è¨€æŸ¥è¯¢**ï¼šç”¨å®Œæ•´å¥å­è¡¨è¾¾
  - ç°ä»£æœç´¢å¼•æ“ï¼ˆå¦‚ChatGPTã€Siriï¼‰æ”¯æŒè¯¥æ–¹å¼ï¼Œä½†ç†è§£èƒ½åŠ›æœ‰é™

## 8. Role of IR

IRç³»ç»Ÿçš„ä»»åŠ¡æ˜¯ï¼š

- æä¾›ä¸€ç»„**ç›¸å…³æ–‡æ¡£**ï¼ˆé€šå¸¸æŒ‰ç›¸å…³æ€§æ’åºï¼‰
- ç”¨æˆ·éœ€è‡ªè¡Œé˜…è¯»è·å–çŸ¥è¯†

## 9. Why IR?

- **Information Overload**

## 10. Relevance

- satisfies the user's information need. ï¼ˆæ»¡è¶³ç”¨æˆ·ä¿¡æ¯éœ€æ±‚ï¼‰
- Subjective ä¸»è§‚
- Remember that a query is just an expression of an information need. ï¼ˆåªæ˜¯Information needçš„è¡¨è¾¾ï¼Œä¸ä¸€å®šæ—¶çœŸå®éœ€æ±‚ï¼‰

## 11. Evaluate

- Documents returns satisfy the information need.
- ç”¨æˆ·æ˜¯åˆ¤æ–­æ ‡å‡†ï¼Œä½†è¯„ä¼°å›°éš¾
- æ„å»ºæµ‹è¯•é›†åˆ+æ ‡å‡†æŸ¥è¯¢+ä¸“å®¶åˆ¤æ–­æ–‡æ¡£æ˜¯å¦ç›¸å…³

## 12. Related Research

1. **Question Answering (é—®ç­”ç³»ç»Ÿ)**
2. **ä¿¡æ¯æŠ½å–ï¼ˆInformation Extractionï¼‰**



# 01a-IR-Example: Boolean Queries

## 1. Boolean style query

> Brutus AND Caesar NOT Calpurnia

**ç¬¬ä¸€ç§â€œæœ´ç´ â€åšæ³•**æ˜¯é€è¡Œè¯»å–æ‰€æœ‰å‰§æœ¬ï¼š

- ç­›é€‰å‡ºåŒæ—¶åŒ…å«â€œBrutusâ€å’Œâ€œCaesarâ€çš„å‰§æœ¬ï¼›
- å†æ’é™¤åŒ…å«â€œCalpurniaâ€çš„å‰§æœ¬ã€‚

**é—®é¢˜**ï¼š

- **Slow æ•ˆç‡ä½ä¸‹**ï¼šå¯¹äºå¤§è§„æ¨¡è¯­æ–™ï¼ˆcorpusï¼‰éå¸¸ç¼“æ…¢ã€‚
- **Other operations not feasible æ— æ³•æ‰©å±•**ï¼šä¸æ”¯æŒå¦‚â€œè¯é¡¹æ¥è¿‘â€ä¹‹ç±»çš„å¤æ‚æ“ä½œã€‚
- **Ranked retrieval not possibleä¸èƒ½æ’åº**ï¼šæ— æ³•è¿”å›â€œæœ€ç›¸å…³â€çš„æ–‡æ¡£ï¼ˆæ— æ’åæœºåˆ¶ï¼‰ã€‚

## 2. Term-Document Incidence Matrixï¼ˆè¯é¡¹-æ–‡æ¡£çŸ©é˜µï¼‰

ç¤ºä¾‹çŸ©é˜µå¦‚ä¸‹ï¼š

| Term      | A&C  | JC   | Tempest | Hamlet | Othello | Macbeth |
| --------- | ---- | ---- | ------- | ------ | ------- | ------- |
| Antony    | 1    | 1    | 0       | 0      | 0       | 1       |
| Brutus    | 1    | 1    | 0       | 1      | 0       | 0       |
| Caesar    | 1    | 1    | 0       | 1      | 1       | 1       |
| Calpurnia | 0    | 1    | 0       | 0      | 0       | 0       |
| Cleopatra | 1    | 0    | 0       | 0      | 0       | 0       |

> æ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªæ–‡æ¡£ï¼ˆå‰§æœ¬ï¼‰ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè¯é¡¹ã€‚1 è¡¨ç¤ºè¯¥å‰§æœ¬ä¸­åŒ…å«è¯¥è¯é¡¹ï¼Œ0 è¡¨ç¤ºä¸åŒ…å«ã€‚

## 3. Incidence vectors
æ¯ä¸ªè¯é¡¹å¯ä»¥è½¬åŒ–ä¸ºä¸€ä¸ªâ€œå­˜åœ¨å‘é‡â€ï¼š

- Brutus:â€ƒâ€ƒ110100
- Caesar:â€ƒâ€ƒ110111
- Calpurnia:â€ƒ010000

è¿™äº›äºŒè¿›åˆ¶å‘é‡å¯ç”¨äºåç»­å¸ƒå°”è¿ç®—ã€‚

## 4. Operators

> Brutus AND Caesar NOT Calpurnia

1. **å–åæ“ä½œ**ï¼ˆNOT Calpurniaï¼‰ï¼š

- åŸå‘é‡ï¼šâ€ƒ010000
- å–ååï¼šâ€ƒ101111

2. **ä½ä¸æ“ä½œ**ï¼ˆANDï¼‰

   ```makefile
   Brutus:â€ƒâ€ƒ110100  
   Caesar:â€ƒâ€ƒ110111  
   NOT Calpurnia: 101111  
   â†’
   110100 AND 110111 AND 101111 = 100100
   ```

   

3. **è§£é‡Šç»“æœ**ï¼š1 è¡¨ç¤ºç›¸å…³çš„æ–‡æ¡£ã€‚å¯¹åº”çš„æ˜¯ç¬¬1å’Œç¬¬4éƒ¨å‰§æœ¬ï¼š

   - Antony and Cleopatraâ†³
   - Hamlet



## Bigger collections

å‡è®¾æˆ‘ä»¬æœ‰ï¼š

- N = 1,000,000 æ–‡æ¡£ï¼Œæ¯ä¸ªåŒ…å«çº¦ 1,000 ä¸ªè¯ï¼›
- æ¯è¯å¹³å‡6å­—èŠ‚ï¼Œæ€»æ•°æ®é‡ = 6GBï¼›
- M = 500,000 ä¸ªä¸åŒè¯é¡¹ã€‚

åˆ™ term-document matrix å¤§å°ä¸ºï¼š

```
500,000 Ã— 1,000,000 = 5Ã—10^11 bitsï¼ˆå³5000äº¿ä½ï¼‰
```

ç„¶è€Œå®é™…ä¸Šå¤§éƒ¨åˆ†å€¼æ˜¯0ï¼Œå› æ­¤è¯¥çŸ©é˜µæåº¦ç¨€ç–ï¼ˆsparseï¼‰



## Ambiguous Queries

å³ä½¿æ˜¯æ˜ç¡®çš„æŸ¥è¯¢è¯ï¼Œä¹Ÿå¯èƒ½å­˜åœ¨å¤šç§å«ä¹‰ã€‚ä¾‹å¦‚ï¼š

- æŸ¥è¯¢è¯ "jaguar"ï¼š
  - å¯¹æŸäº›ç”¨æˆ·æ˜¯åŠ¨ç‰©ï¼›
  - å¯¹å¦ä¸€äº›ç”¨æˆ·æ˜¯æ±½è½¦å“ç‰Œã€‚
- æŸ¥è¯¢è¯ "bank"ï¼š
  - æ²³å²¸ï¼ˆriver bankï¼‰ï¼›
  - é“¶è¡Œï¼ˆfinancial institutionï¼‰ï¼›
  - é£æœºè½¬å¼¯åŠ¨ä½œï¼ˆé£è¡Œæœ¯è¯­ï¼‰ã€‚

# 01b-Indexing

## 1. Introduction: Indexing

A data structure used to representation called index. The process of create index is called indexing.

## 2. Inverted Indexï¼ˆå€’æ’ç´¢å¼•ï¼‰(2024å¹´è€ƒå¯Ÿ)

å€’æ’ç´¢å¼•æ˜¯ä¸€ç§å°†æ¯ä¸ªè¯é¡¹ï¼ˆtermï¼‰æ˜ å°„åˆ°å…¶å‡ºç°è¿‡çš„æ–‡æ¡£IDï¼ˆdocIDï¼‰åˆ—è¡¨çš„ç»“æ„ã€‚

- **Postings list**ï¼šæ¯ä¸ª term å¯¹åº”ä¸€ä¸ªæ–‡æ¡£IDé›†åˆï¼Œç§°ä¸º posting listï¼›
- **Posting**ï¼šå…¶ä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œå³ä¸€ä¸ªæ–‡æ¡£IDï¼›
- **Dictionary**ï¼šåŒ…å«æ‰€æœ‰è¯é¡¹çš„æœ‰åºåˆ—è¡¨ï¼›
- åˆ—è¡¨é€šå¸¸æŒ‰ docID æ’åºï¼Œæ–¹ä¾¿åç»­æ“ä½œï¼ˆå¦‚åˆå¹¶ã€äº¤é›†ï¼‰ã€‚

## 3. Construction (2024å¹´è€ƒå¯Ÿ)

**Step 1ï¼šåˆ†è¯ï¼ˆTokenisationï¼‰**

- To (token, docID)
- punctuation is removed
- convert to lowercase

**Step 2ï¼šæ’åºï¼ˆSortingï¼‰**

- Sort by terms then by docIDs

**Step 3ï¼šæ„å»ºå­—å…¸å’Œå€’æ’åˆ—è¡¨**

- merge: a term appears more than once in a document
- Split into a dictionary and posting
- Also record **document frequency** of each term

## 4. Process Query

> Brutus AND Caesar

1. Locate Brutus and Caesar in the dictionary and retrieve its postings
2. Merge the 2 postings

## 5. å¸ƒå°”æ“ä½œ (2023å¹´è€ƒå¯Ÿ)

- AND(äº¤é›†)ï¼š AND is used to narrow a search. More AND, Fewer records.
- OR(å¹¶é›†)ï¼š OR is used to broaden a search. Documents containing any number of the terms specified will be returned
- NOT(å·®é›†)ï¼š NOT is used to specifically exclude a term from search. More NOT, Fewer records.

## 6. Postings List çš„åˆå¹¶ï¼ˆMergingï¼‰

**Intersectionï¼ˆäº¤é›†ï¼‰ç®—æ³•**

- ä¸¤ä¸ªå·²æ’åºçš„docIDåˆ—è¡¨ï¼Œä»å¤´åˆ°å°¾çº¿æ€§æ‰«æ
- æ—¶é—´å¤æ‚åº¦: O(x+y)

# 01c-Query Potimisationï¼ˆ2023è€ƒå¯Ÿï¼‰

> Brutus AND Calpurnia AND Caesar

æ­£å¸¸æ­¥éª¤ï¼šFor each of the terms, retrieve its posting list and merge with the others.

**ä¼˜åŒ–ï¼š** Process in order of increasing frequency

- Start with the smallest set
- åŸå› ï¼šIf we reach the end of shorter list, we can stop!

____

**æ›´å¤æ‚çš„æŸ¥è¯¢**

> (madding OR crowd) AND (ignoble OR strife)

- Get the frequency for all terms
- Estimate the size of each OR
- Process in increasing order of OR size

**Skip Pointer** è·³è¡¨ä¼˜åŒ–

åŸºæœ¬æ€è·¯ï¼š

- Introduce skip poiner has a special type of pointer, it can reach later parts of list without interating through every element.

**Skip Poiner çš„ä½ç½®**

æƒè¡¡(Tradeoff):

- More Skips --> shorter  skip spans but lots of comporisons to skip pointers.
- Fewer Skips --> few pointer comparison, but then long skip spans

**å¦‚æœ postings list é•¿åº¦ä¸º Lï¼Œå»ºè®®æ’å…¥ âˆšL ä¸ªç­‰é—´éš”è·³è¡¨**



# 02a-Preprocessing

## 1. Tokenisation(åˆ†è¯)

**åŸºæœ¬æ¦‚å¿µ**

- Token is an instance of a sequence of characters. **Not same with words**
- We get Token after futher processing

**Issues**

**â€œFinlandâ€™s capitalâ€** â†’ â€œFinlandâ€ å’Œ â€œâ€™sâ€ åº”è¯¥åˆå¹¶å—ï¼Ÿ

**â€œMercedes-Benzâ€** â†’ æ˜¯ä¸€ä¸ªè¯é¡¹è¿˜æ˜¯ä¸¤ä¸ªï¼Ÿ

**â€œstate-of-the-artâ€** â†’ å¦‚ä½•å¤„ç†è¿å­—ç¬¦ï¼Ÿ

**â€œSan Franciscoâ€** â†’ æ˜¯ä¸€ä¸ªè¯é¡¹è¿˜æ˜¯ä¸¤ä¸ªï¼Ÿ

## 2. language issues(2024è€ƒå¯Ÿ)

**French**

L'ensemble -> one token or two?

**German**

Lebensversicherungsgesellschaftsangestellter

**Chinese/Japanese**

- No spaces between word.
- may be ambiguous
- multiple alphabets intermingled

**Arabic / Hebrew**

written right to left

## 3. Normalisation

same semantics but different forms into the same form.

**å¸¸è§ç­–ç•¥**

- Changing all tokens to lowercase
- Delete full stops
- Delete hyphens

## 4. Thesauri and Soundexï¼ˆåŒä¹‰è¯ä¸è¯­éŸ³ç´¢å¼•ï¼‰

 **Thesaurusï¼ˆè¯åº“ï¼‰**

- æ„å»º**åŒä¹‰è¯ç±»ç­‰ä»·è¯é¡¹**ï¼šï¼ˆSynonyms, homonyms)
  - car = automobileâ†³
  - color = colourâ†³
- å¯åœ¨æ–‡æ¡£ä¸­å°†â€œautomobileâ€ç´¢å¼•ä¸ºâ€œcar-automobileâ€ï¼Œæˆ–åœ¨æŸ¥è¯¢ä¸­æ‰©å±•ã€‚

**Soundexï¼ˆè¯­éŸ³ç´¢å¼•ï¼‰**

- å¤„ç†æ‹¼å†™é”™è¯¯çš„è¯é¡¹ï¼›
- æ ¹æ®å‘éŸ³ç‰¹å¾å¯¹è¯é¡¹åˆ†ç±»ï¼Œè€ŒéæŒ‰æ‹¼å†™ï¼›
- å¸¸ç”¨äºäººååŒ¹é…ã€è¯­éŸ³æœç´¢ç­‰åœºæ™¯ã€‚

# 02b-Stopwords, Stemming and Lemmatisation

**Stopword Removal**ï¼ˆåœç”¨è¯å»é™¤ï¼‰

**Stemming**ï¼ˆè¯å¹²æå–ï¼‰

**Lemmatisation**ï¼ˆè¯å…ƒè¿˜åŸï¼‰

## 1,. Stopword Removalï¼ˆå»é™¤åœç”¨è¯ï¼‰(2024å¹´è€ƒå¯Ÿ)

**å®šä¹‰**

- Stopword is a commonly occurring term that appears in so many documents that it does not add to the meaning of the document.
- the, of a, to
- Common appear in almost all document, and of little use when differentiating between documents

**ä¸ºä½•å»é™¤**

- Reduce number of term
- Stopword Removalï¼ˆå»é™¤åœç”¨è¯ï¼‰

**ä½¿ç”¨ Zipfâ€™s Law è¾¨è¯†åœç”¨è¯ï¼š**

- Stopword can be estimated using Zipf's Law
- Only a few terms are used very often, more are used ont rarely
- ç¬¬2å¸¸ç”¨è¯é¢‘ â‰ˆ ç¬¬1çš„Â½ï¼Œç¬¬3å¸¸ç”¨ â‰ˆ ç¬¬1çš„â…“ï¼Œä¾æ­¤ç±»æ¨ï¼›

**æƒè¡¡ï¼ˆTradeoffï¼‰ï¼š**

- **å¥½å¤„**ï¼šsamll space and little timeï¼›
- **é£é™©**ï¼šå¯èƒ½å»é™¤è¯­ä¹‰é‡è¦çš„è¯ï¼Œå¦‚ï¼š
  - â€œKing of Denmarkâ€
  - â€œTo be or not to beâ€
  - â€œFlights to Londonâ€
- **Solution:**
  - Don's remove stopwords
  - Recognise when combinations of stopwords are meaningful, and include these as terms in the index.

## 2. Stemming ï¼ˆè¯å¹²æå–ï¼‰ (2024è€ƒå¯Ÿï¼Œ 2023è€ƒå¯Ÿ)

**å®šä¹‰**

Stemming is the process that maps terms to a common root.

Stemming is also known as suffix stripping

**Problems**

- **Overstemming**: sometimes suffixes can be removed from word that are not related
- **Homographsï¼ˆåŒå½¢å¼‚ä¹‰è¯ï¼‰**

## 3. Lemmatisationï¼ˆè¯å…ƒè¿˜åŸï¼‰(2024è€ƒå¯Ÿ, 2023è€ƒå¯Ÿ)

**å®šä¹‰**

- Lemmatisation is a NLP technique for converting word into lemmas. (a reak word)
- Slower than stemming
- More accuracy

# 03a-Phrase Queries

Want to answer queries such as "stanford university" as phrase

## 1. Biword indexs (åŒè¯ç´¢å¼•)

**åŸºæœ¬æ€è·¯**

- Index every consecutiive pair of terms(æ¯ä¸¤ä¸ªè¿ç»­å•è¯) in the text as a phrase
- ä¾‹å¦‚ï¼šâ€œFriends, Romans, Countrymenâ€ ç”Ÿæˆçš„ biwords ä¸ºï¼š
  - friends romans
  - romans countrymen

**é—®é¢˜**

- False positives
- Index blowup due tob bigger dictionary

## Positional Indexesï¼ˆä½ç½®ç´¢å¼•ï¼‰(2023å¹´è€ƒå¯Ÿ)

 **å®šä¹‰ï¼š**

ä½ç½®ç´¢å¼•åœ¨æ¯ä¸ªå€’æ’è®°å½•ä¸­ä¸ä»…è®°å½•æ–‡æ¡£IDï¼Œè¿˜è®°å½•**è¯é¡¹åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®**ã€‚

æ•°æ®ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

```
<term, number of docs;
 doc1: position1, position2, ...;
 doc2: position1, position2, ...;
 ... >
```

 **ä½œç”¨ï¼š**

é€šè¿‡è¯é¡¹å‡ºç°çš„ä½ç½®ï¼Œå¯ä»¥åˆ¤æ–­ä¸¤ä¸ªæˆ–å¤šä¸ªè¯é¡¹æ˜¯å¦**è¿ç»­**å‡ºç°ï¼Œä»è€Œæ”¯æŒä»»æ„é•¿åº¦çš„çŸ­è¯­æŸ¥è¯¢ã€‚

## æˆæœ¬

**Positional Index**

- Expand postings storage
- A positional index is 2â€“4 times as large as a non-positional index.
- Positional index size 35â€“50% of volume of original text

# 03b-Vector-Space-Model-Introduction

## 1. Boolean Model çš„ç¼ºç‚¹

- Either relevant or non-relevant to the query
- No partial match to query conditions lead to too few documents being retrieved.
- No Ranking

## 2. Introduction to Vector Space Model

**This model both documents and queries in an N-dimensional space**

>  VSM å°†æ–‡æ¡£å’ŒæŸ¥è¯¢è¡¨ç¤ºä¸ºN-dimensional space

## 3. Vector Space Basisï¼ˆå‘é‡ç©ºé—´çš„æ„å»ºåŸºç¡€ï¼‰

 **æ ¸å¿ƒæ¦‚å¿µï¼šBag of Wordsï¼ˆè¯è¢‹æ¨¡å‹ï¼‰**

- æ¯ä¸ªæ–‡æ¡£è¢«è§†ä¸ºä¸€ä¸ªè¯é¡¹é›†åˆï¼ˆæ— åºï¼‰ï¼Œå¿½ç•¥è¯åºï¼›
- é›†åˆä¸­æ¯ä¸ª**å”¯ä¸€è¯é¡¹**æ„æˆå‘é‡ç©ºé—´çš„ä¸€ä¸ªç»´åº¦ Each unique term in the collection is represented by a dimension in the vector spaceï¼›
- å¦‚æœä¸€ä¸ªæ–‡æ¡£åŒ…å«æŸä¸ªè¯é¡¹ï¼Œè¯¥ç»´åº¦å–å€¼ä¸ºå¯¹åº”çš„**æƒé‡ï¼ˆterm weightï¼‰**ï¼›
- è‹¥æ–‡æ¡£ä¸åŒ…å«æŸè¯é¡¹ï¼Œè¯¥ç»´åº¦ä¸º 0ï¼›
- æœ€ç»ˆé€šè¿‡å‘é‡ä¹‹é—´çš„è·ç¦»æ¥è®¡ç®—ç›¸ä¼¼åº¦ similarity scoreã€‚

## 4. Partial Matching and Rankingï¼ˆéƒ¨åˆ†åŒ¹é…ä¸æ’åºï¼‰

**VSM facilitates partial matching by using non-binary term weights.**

$sim(q,d)$: è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸ä¼¼åº¦å¾—åˆ†

Returna ranked list of docs

## 5. Document and Query Representation

**ç¤ºä¾‹è¯­æ–™ï¼š**

1. â€œ**Information** **Retrieval** is an **exciting** **subject**â€
2. â€œ**Mathematics** is **important** in **Information** **Retrieval**â€

 **å»é™¤åœç”¨è¯åæå–çš„ 6 ä¸ªè¯é¡¹ï¼š**

- information
- retrieval
- exciting
- subject
- mathematics
- important

 **å‘é‡ç©ºé—´ç»´åº¦ = 6ï¼ˆæ¯ä¸ªè¯é¡¹ä¸€ä¸ªç»´åº¦ï¼‰**

____



**Document Representation**

é€‰å–terms, æ ¹æ®termsçš„æ•°é‡å†³å®šå‘é‡çš„ç»´åº¦ã€‚

é‡‡ç”¨ç®€åŒ–çš„æƒé‡æ–¹æ¡ˆï¼š
$$
w_{i, j}= \begin{cases}1, & \text { if term } i \text { is in document } j \\ 0, & \text { otherwise }\end{cases}
$$

| Term        | Doc 1 | Doc 2 |
| ----------- | ----- | ----- |
| information | 1     | 1     |
| retrieval   | 1     | 1     |
| exciting    | 1     | 0     |
| subject     | 1     | 0     |
| mathematics | 0     | 1     |
| important   | 0     | 1     |



**è¯´æ˜ï¼š**æ‰€æœ‰æ–‡æ¡£å‘é‡éƒ½æ˜¯ 6 ç»´çš„ï¼Œæœªå‡ºç°çš„è¯é¡¹å–å€¼ä¸º 0ã€‚

____



 **Query Representation**

> important information

(1, 0, 0, 0, 0, 1)

éœ€è¦å¯¹åº”è¯è¡¨çš„é¡ºåº. Order of the terms in the vector is the same as corpus

## 03c-Vector-Space-Model-Cosine-Similarity

åŸç†ï¼šThe smaller the angle between two vectors, the more similar they are.
$$
\vec{a} \cdot \vec{b}=|\vec{a}||\vec{b}| \cos \theta
$$

## 1. Why Use Cosine Similarity?

**èŒƒå›´æ¸…æ™°**ï¼šå› ä¸ºæ‰€æœ‰æƒé‡éè´Ÿï¼ˆå¦‚ TFã€TF-IDFï¼‰ï¼Œè§’åº¦åœ¨ [0Â°, 90Â°]ï¼›

- ç›¸ä¼¼åº¦æœ€å¤§ï¼ˆcos 0Â° = 1ï¼‰ï¼šå®Œå…¨ä¸€è‡´ï¼›
- ç›¸ä¼¼åº¦æœ€å°ï¼ˆcos 90Â° = 0ï¼‰ï¼šå®Œå…¨æ— å…³ï¼›

**æ˜“äºè®¡ç®—å’Œè§£é‡Š**ï¼›

**é€‚ç”¨äºç¨€ç–å‘é‡**ï¼šå®é™…æ–‡æ¡£å‘é‡ç»´åº¦æé«˜ï¼ˆæ•°ä¸‡ï¼‰ï¼Œä½†å¤§éƒ¨åˆ†ä¸º 0

## 2. Cosine Similarity

$$
\operatorname{sim}\left(d_j, q\right)=\frac{\vec{d}_j \cdot \vec{q}}{\left|\vec{d}_j\right||\vec{q}|}
$$

$$
\operatorname{sim}\left(d_j, q\right)=\frac{\sum_{i=1}^T w_{i, j} \cdot w_{i, q}}{\sqrt{\sum_{i=1}^T w_{i, j}^2} \cdot \sqrt{\sum_{i=1}^T w_{i, q}^2}}
$$

- Tæ˜¯è¯­æ–™åº“ä¸­ä¸åŒè¯é¡¹çš„æ€»æ•°
- $w_{i,j}$: è¯i åœ¨æ–‡æ¡£jä¸­çš„æƒé‡
- $w_{i,j}$: è¯ i åœ¨æŸ¥è¯¢ q ä¸­çš„æƒé‡

![image-20250528115107776](./image-20250528115107776.png)

## 3. Some Questions

å‡ ä¸ªå…³é”®é—®é¢˜å¸®åŠ©å®é™…å®ç°ï¼š

- è‹¥æŸ term ä¸åœ¨æŸ¥è¯¢ä¸­ï¼Œå¯¹ç›¸ä¼¼åº¦ **æ²¡æœ‰å½±å“ï¼ˆä¹˜ç§¯ä¸º 0ï¼‰**ï¼›
- è‹¥ term ä¸åœ¨æ–‡æ¡£ä¸­ï¼Œä¹Ÿä¸ä¼šå½±å“ç‚¹ç§¯ï¼›
- å‘é‡ç»´åº¦å§‹ç»ˆå›ºå®šï¼ˆè¯é¡¹æ€»æ•°ï¼‰ï¼Œä½†å®é™…ä¸º**ç¨€ç–è¡¨ç¤º**ï¼ˆsparse representationï¼‰ï¼›
- å®é™…ä¸­æˆ‘ä»¬ä¸æ˜¾å¼å­˜å‚¨å®Œæ•´å‘é‡ï¼Œè€Œä½¿ç”¨å€’æ’ç´¢å¼•è®°å½•éé›¶ç»´åº¦ã€‚

# 03d-Vector-Space-Model-Term-Weights

Term Frequency(TF)

Inverse Document Frequency(IDF)

## 1. Term  Frequency (TF)

The more frequently a term appears, the greater its importance to the document should be.

## 2. ä¸ºä»€ä¹ˆé¢‘ç‡ä¸èƒ½ç›´æ¥ç”¨

- A word that appears in every documents is not very useful.(ç±»ä¼¼stopwords)
- Long documents naturally contain more terms;

## 3. TF-IDF æƒé‡æ¨¡å‹

**TF (Term Frequency)**ï¼š

- è¡¡é‡æŸä¸ªè¯é¡¹åœ¨**å½“å‰æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡**ï¼›
- é¢‘ç‡è¶Šé«˜ï¼Œä»£è¡¨è¯¥è¯å¯¹æ–‡æ¡£å†…å®¹è¶Šé‡è¦ã€‚

**IDF (Inverse Document Frequency)**ï¼š

- è¡¡é‡æŸä¸ªè¯é¡¹åœ¨**æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­å‡ºç°çš„ç¨€æœ‰åº¦**ï¼›
- è¶Šå°‘æ–‡æ¡£åŒ…å«æŸè¯ï¼Œè¯´æ˜å®ƒè¶Šæœ‰åŒºåˆ†åŠ› â†’ æƒé‡åº”æ›´é«˜ã€‚

$$
w_{i, j}=t f_{i, j} \cdot i d f_i
$$

## 4. Term Frequencyï¼ˆTFï¼‰å…¬å¼

$$
t f_{i, j}=\frac{\text { freq }_{i, j}}{\max \text { freq }_j}
$$

å…¶ä¸­ï¼š

- $freq_{i,j}$ï¼šè¯é¡¹ iåœ¨æ–‡æ¡£ j ä¸­å‡ºç°çš„æ¬¡æ•°ï¼›
- $\max freq_j$â€‹ï¼šæ–‡æ¡£ j ä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„è¯çš„é¢‘ç‡ã€‚

ç›®çš„ï¼š

- Ensure that terns in long documents do not get an unfair advantage

## 5. Inverse Document Frequencyï¼ˆIDFï¼‰å…¬å¼

$$
i d f_i=\log \frac{N}{n_i}
$$

å…¶ä¸­ï¼š

- $N$ï¼šæ•´ä¸ªæ–‡æ¡£é›†åˆä¸­çš„æ–‡æ¡£æ€»æ•°ï¼›
- $n_i$ï¼šåŒ…å«è¯é¡¹ $i$ çš„æ–‡æ¡£æ•°ã€‚

ç›®çš„ï¼š

- Give a higher weight to terms that are rare



## **6ï¼šTF-IDF æƒé‡**

$$
w_{i, j}=t f_{i, j} \cdot i d f_i
$$

**æ¯ä¸ªè¯é¡¹åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„æƒé‡éƒ½æ˜¯å”¯ä¸€çš„**ï¼Œå›  TF éšæ–‡æ¡£è€Œå˜ï¼ŒIDF ä»…éšè¯é¡¹åœ¨æ•´ä¸ªé›†åˆä¸­çš„åˆ†å¸ƒè€Œå˜ã€‚

# 03e-Vector_Space_Model_Conclusions

## **1. TF-IDF Advantages:**

- TF gives more influence to a document that contains many occurrences of a particular term
- IDF lessens the impact of terms that appear very often in the collection.

## 2. å‘é‡ç©ºé—´æ¨¡å‹çš„ä¼˜ç¼ºç‚¹

Advantages:

- Term weighting can improve retrieval performance
- Partial matching strategy allows for retrieval of documents taht match part of the query
- consine similarity can be used to return ranked list of docs

Disadvantages:

- Assumption terms are independent, which sometimes harm performance



##3.  **å‘é‡ç©ºé—´æ¨¡å‹çš„å®ç°æ­¥éª¤ï¼ˆImplementationï¼‰**

1. **è¯»å–æ–‡æ¡£ï¼ˆRead in documentsï¼‰**
2. **åˆ†è¯ï¼ˆTokenisationï¼‰**
3. **å»é™¤åœç”¨è¯ï¼ˆStopword Removalï¼‰**
4. **è¯å¹²æå–ï¼ˆStemmingï¼‰**
5. **åŠ å…¥ç´¢å¼•ç»“æ„ï¼ˆAdd to indexï¼‰**
6. **è®¡ç®— TF-IDF æƒé‡**
   - æ¯ä¸ªè¯é¡¹å…·æœ‰ä¸€ä¸ª IDF å€¼ï¼ˆå…¨å±€ï¼‰ï¼›
   - æ¯ä¸ªæ–‡æ¡£ä¸­æ¯ä¸ªè¯é¡¹æœ‰è‡ªå·±çš„ TF å€¼ï¼ˆå±€éƒ¨ï¼‰ã€‚
7. **æ¥æ”¶æŸ¥è¯¢ï¼ˆReceive queryï¼‰**
8. **è¿”å›æŸ¥è¯¢ç»“æœï¼ˆRetrieve resultsï¼‰**

è¯¥æµç¨‹æ„æˆäº†ç°ä»£ IR ç³»ç»ŸåŸºäºå‘é‡ç©ºé—´æ¨¡å‹çš„æ ‡å‡†å¤„ç†æ¡†æ¶ã€‚

# 08a Fusion

## 1. ä¸ºä»€ä¹ˆç ”ç©¶Fusion

   - IRç®—æ³•å¤šè€Œå¤æ‚
   - å°†**å¤šå¥—æ£€ç´¢ç³»ç»Ÿ**ï¼ˆæˆ–å¤šç§ç®—æ³•ï¼‰å¯¹åŒä¸€æŸ¥è¯¢è¿”å›çš„ç»“æœæ•´åˆæˆ**å•ä¸€çš„æ’åºåˆ—è¡¨**ï¼ŒæœŸæœ›æ•´ä½“è´¨é‡ä¼˜äºä»»ä¸€è¾“å…¥åˆ—è¡¨ã€‚
   - å¬å›ç‡å’Œç²¾ç¡®ç‡å¥½

## 2. åº”ç”¨åœºæ™¯
   - Metasearch
   - Distributed IR
   - Internal Metasearch: è®¸å¤šç®—æ³•å¯¹åŒä¸€æ–‡æ¡£è¿›è¡Œæœç´¢

## 3. **Corpus Overlap** ï¼šç´¢å¼•é‡å ç¨‹åº¦ä¸ç­–ç•¥

1. **Disjoint databases**ï¼ˆå®Œå…¨ä¸é‡å ï¼‰
   - åªå¯èƒ½å‡ºç°åœ¨ä¸€ä¸ªç»“æœé›†ä¸­
   - åˆç§° *Collection Fusion*ï¼Œå¸¸è§äº Distributed IRâ†³
2. **Identical databases**ï¼ˆå®Œå…¨ç›¸åŒï¼‰â†³
   - æ–‡æ¡£é¢‘ç¹å‡ºç°åœ¨å¤šä¸ªåˆ—è¡¨
   - é‡ç‚¹ä¾èµ–â€œå¤šä¸ªç³»ç»ŸåŒæ—¶è®¤åŒâ€è¿™ä¸€ä¿¡å· â†’ **Data Fusion**ï¼ˆæœ¬æ–‡ä¸»è§’ï¼‰08a-Fusion-Introduction08a-Fusion-Introduction
3. **Overlapping databases**ï¼ˆéƒ¨åˆ†é‡å ï¼‰â†³
   - æ—¢æœ‰ç‹¬å æ–‡æ¡£ï¼Œä¹Ÿæœ‰å…¬å…±æ–‡æ¡£
   - åˆ¤æ–­â€œæ²¡å‡ºç°â€æ˜¯â€œçœŸçš„ä¸ç›¸å…³â€è¿˜æ˜¯â€œç³»ç»Ÿæ²¡æ”¶å½•â€ä¼šå˜å¾—æ£˜æ‰‹08a-Fusion-Introduction08a-Fusion-Introduction

**æç¤ºï¼š** å†³å®šèåˆç­–ç•¥å‰ï¼Œå¿…é¡»å…ˆå¼„æ¸…å„ç³»ç»Ÿç´¢å¼•çš„ç›¸ä¼¼åº¦ â€”â€” å½±å“â€œåˆå”±æ•ˆåº”â€ï¼ˆè§ä¸‹ï¼‰æƒé‡ã€‚

## 4. ä¸‰å¤§ç»å…¸â€œæ•ˆåº”â€ (Effects)

1. ### Skimming Effect

   â€œå¥½ç³»ç»ŸæŠŠç›¸å…³æ–‡æ¡£æ’åœ¨å‰é¢â€ â†’ åªè¦**æˆªå–å„åˆ—è¡¨å‰ N æ¡**å†èåˆï¼Œå¸¸èƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚æ‰€æœ‰ä¸»æµç®—æ³•éƒ½é»˜è®¤åˆ©ç”¨å®ƒ08a-Fusion-Introduction08a-Fusion-Introductionã€‚

2. ### Chorus Effect

   å¦‚æœ**å¤šä¸ªç³»ç»ŸåŒæ—¶**æŠŠæŸæ–‡æ¡£æ’å¾—å¾ˆé å‰ï¼ˆæˆ–è‡³å°‘éƒ½è¿”å›ï¼‰ï¼Œé‚£æ˜¯â€œç›¸å…³æ€§â€çš„å¼ºä½è¯ï¼Œåº”ç»™å‡ºæ›´é«˜èåˆå¾—åˆ†ã€‚ç¤ºä¾‹ä¸­ d5 è¢«ä¸¤ç³»ç»ŸåŒæ—¶æ’å‰äºŒï¼Œä¼˜å…ˆçº§è‡ªç„¶é«˜äºåªè¢«ä¸€ä¸ªç³»ç»Ÿçœ‹å¥½çš„ d19

   - åœ¨**å®Œå…¨ç›¸åŒç´¢å¼•**åœºæ™¯æœ€æœ‰æ•ˆï¼›åœ¨ Disjoint åœºæ™¯å®Œå…¨æ— ç”¨ã€‚

3. ### Dark Horse Effect

   æŸä¸ªç³»ç»Ÿè´¨é‡å¼‚å¸¸é«˜æˆ–ä½ï¼Œå¯èƒ½å€¼å¾—å•ç‹¬ä¿¡èµ–æˆ–å¿½ç•¥ã€‚ä½†**éš¾ä»¥è‡ªåŠ¨ä¾¦æµ‹**ï¼Œå¤šæ•°èåˆç®—æ³•ä¸ä¼šæ˜¾å¼åˆ©ç”¨

## 5. **Data Fusion ç®—æ³•ä¸‰å¤§ç±»*

1. **Rank-based Fusion**
   - ä»£è¡¨ï¼š**CombSUM (å–å€’æ•°æ’åå’Œ)**ã€**BordaFuse** ç­‰
   - åªä¾é åæ¬¡ï¼Œç®€å•æ˜“ç”¨ï¼›å¸¸ä¼´éšâ€œæŠ•ç¥¨â€æ€æƒ³ã€‚
2. **Score-based Fusion**
   - ä»£è¡¨ï¼š**CombMNZï¼ˆå½’ä¸€åŒ–åˆ†æ•°æ±‚å’Œå¹¶ä¹˜å‡ºç°æ¬¡æ•°ï¼‰**ã€**ProbFuse**
   - éœ€æŠŠä¸åŒç³»ç»Ÿçš„åŸå§‹åˆ†æ•°åšå½’ä¸€åŒ–å¤„ç†ã€‚
3. **Segment-based Fusion**
   - ä»£è¡¨ï¼š**SlideFuse**ã€**Rank-Biased Precision Fusion**â†³
   - å°†åˆ—è¡¨åˆ’æ®µååˆ†é…æƒé‡ï¼Œå…¼é¡¾ Skimming ä¸ä½ç½®é²æ£’æ€§ã€‚

# 08-b Rank-Based Fusion

## 1. Rank-Based Fusion æ¦‚è§ˆ

> â€œåªçœ‹åæ¬¡ï¼Œä¸çœ‹åˆ†æ•°â€

- **åŠ¨æœº**ï¼šåœ¨å¾ˆå¤šæ£€ç´¢ç³»ç»Ÿï¼ˆå°¤å…¶æ—©æœŸ TREC è¯„æµ‹ï¼‰é‡Œï¼ŒåŸå§‹ç›¸å…³æ€§åˆ†æ•°çš„å°ºåº¦å„å¼‚ï¼Œç”šè‡³ä¸å¯è·å–ï¼›æ­¤æ—¶æœ€ç¨³å¦¥çš„åšæ³•å°±æ˜¯åªåˆ©ç”¨**æ’åä½ç½®ä¿¡æ¯**ã€‚
- **åŸºæœ¬æ€è·¯**ï¼šä¸ºæ¯ç¯‡æ–‡æ¡£ d è®¾è®¡æŸç§ *rank-score*ï¼Œå®ƒåªä¾èµ– d åœ¨å„è¾“å…¥åˆ—è¡¨ä¸­çš„åæ¬¡ï¼ˆæˆ–æ˜¯å¦å‡ºç°ï¼‰ï¼Œç„¶åæŠŠåˆ†æ•°ç›¸åŠ /åŠ æƒåé‡æ–°æ’åºã€‚
- **ä¸ä¸‰å¤§æ•ˆåº”çš„å…³ç³»**
  - *Skimming*ï¼šè¶Šé å‰çš„åæ¬¡ç»™è¶Šé«˜æƒé‡ã€‚
  - *Chorus*ï¼šåŒä¸€æ–‡æ¡£è‹¥å‡ºç°åœ¨å¤šä¸ªåˆ—è¡¨ï¼Œä¼šå¤šæ¬¡ç´¯åŠ åˆ†æ•°ã€‚
  - *Dark Horse*ï¼šçº¯åŸºäºåæ¬¡æ— æ³•è¯†åˆ«â€œé»‘é©¬ç³»ç»Ÿâ€ï¼Œéœ€å¦åŠ æƒé‡æ‰è¡Œã€‚

## 2  Interleavingï¼ˆè½®ç›˜äº¤å‰ï¼‰

| **æ­¥éª¤**                                                     | **è¯´æ˜**          |
| ------------------------------------------------------------ | ----------------- |
| â‘  å–æ‰€æœ‰è¾“å…¥åˆ—è¡¨çš„æŒ‡é’ˆæŒ‡å‘ *rank = 1*                        |                   |
| â‘¡ ä¾æ¬¡ä»æ¯ä¸ªåˆ—è¡¨æŒ‘é€‰å½“å‰æŒ‡é’ˆæ‰€æŒ‡çš„é¦–ä¸ªâ€œæœªå‡ºç°åœ¨ç»“æœä¸­çš„æ–‡æ¡£â€ï¼Œè¿½åŠ åˆ°èåˆåˆ—è¡¨ | å½¢æˆâ€œRound Robinâ€ |
| â‘¢ å°†è¯¥åˆ—è¡¨æŒ‡é’ˆåç§»ä¸€ä½ï¼Œé‡å¤â‘¡ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾æ·±åº¦æˆ–æ‰€æœ‰åˆ—è¡¨è€—å°½ |                   |



- **ç¤ºä¾‹**ï¼šPPT ä¸­ä¸¤å¥—ç³»ç»Ÿ A/B çš„å‰ååç»è¿‡è½®ç›˜äº¤å‰åå¾—åˆ°<br>`d19 â†’ d5 â†’ d14 â†’ d20 â†’ d4 â†’ d7 â†’ d1 â†’ d12 â€¦`
- **ä¼˜ç‚¹**ï¼šå®ç°æç®€ï¼Œæ— éœ€ä»»ä½•é¢„å¤„ç†æˆ–å½’ä¸€åŒ–ã€‚
- **ç¼ºç‚¹**ï¼š
  1. ***è´¨é‡åŒæƒå‡è®¾***â€”â€”é»˜è®¤æ‰€æœ‰ç³»ç»ŸåŒæ ·å¯é ï¼Œé«˜è´¨é‡åˆ—è¡¨ä¼šè¢«ä½è´¨é‡ç»“æœâ€œç¨€é‡Šâ€ï¼›
  2. **ç¼ºå°‘æ¢¯åº¦**â€”â€”rank 1 ä¸ rank 10 å¾—åˆ°çš„â€œè½®æ¬¡â€æƒé‡ä¸€æ ·ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ Skimmingï¼›
  3. **å¯æ‰©å±•æ€§å·®**â€”â€”ç³»ç»Ÿæ•°ç›® m å¢å¤§æ—¶ï¼Œä¸€æ¬¡åªé€‰ m ç¯‡æ–‡æ¡£ï¼Œè¦†ç›–æ·±åº¦æ…¢ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šæ•™å­¦ demoã€æ— æ³•è·å¾—ä»»ä½•ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯æ—¶çš„ baselineï¼›ç”Ÿäº§ç¯å¢ƒå°‘è§ã€‚

## 3 Borda-Fuseï¼ˆBorda è®¡æ•°ï¼‰

### 3.1 ç®—æ³•åŸç†

- æºè‡ª 18 ä¸–çºªæ³•ç±æ•°å­¦å®¶ **Jean-Charles de Borda** æå‡ºçš„â€œå¤šå€™é€‰å°‘é€‰æ°‘â€æŠ•ç¥¨æ³•ã€‚
- **å®šä¹‰**ï¼š
  - å‡è®¾ä¸€å…±æœ‰ **c** ä¸ªä¸é‡å¤çš„æ–‡æ¡£ï¼ˆå€™é€‰äººï¼‰ã€‚
  - åœ¨æŸä¸ªè¾“å…¥ç³»ç»Ÿä¸­ï¼Œæ’ç¬¬ *r* çš„æ–‡æ¡£å¾— **c âˆ’ r + 1** åˆ†ï¼›æœªè¢«æ’åˆ°çš„æ–‡æ¡£ï¼ŒæŠŠè¯¥ç³»ç»Ÿå‰©ä½™åˆ†æ•°å¹³å‡åˆ†ç»™å®ƒä»¬ï¼ˆå…¬å¹³å¤„ç†â€œç¼ºå¸­â€ï¼‰ã€‚
  - èåˆå¾—åˆ† = å„ç³»ç»Ÿåˆ†æ•°æ±‚å’Œï¼›å†æŒ‰é™åºè¾“å‡ºã€‚

### 3.2 è®¡ç®—ç¤ºä¾‹

- ç¤ºä¾‹ä¸­ **c = 14**ï¼›System A è‡ªèº«æ’äº† 10 ç¯‡æ–‡æ¡£ï¼Œæ‰€ä»¥ç»™æ²¡å‡ºç°çš„ 4 ç¯‡æ–‡æ¡£å¹³å‡ **2.5 åˆ†**ï¼ŒSystem B åˆ™ç»™ **3.5 åˆ†**ã€‚

- è®¡ç®—åå‰äº”åä¸º

  ```
  d5 (27)  >  d14 (23)  >  d1 (18)  >  d19 (17.5)  >  d12 (15.5)
  ```

  ç›¸æ¯” Interleavingï¼Œd5/d14 è·ƒå‡è‡³æ¦œé¦–ï¼Œä½“ç°äº† **Chorus Effect**ï¼ˆä¸¤ç³»ç»Ÿå…±åŒé«˜æ’ï¼‰ã€‚

### 3.3 ä¼˜åŠ£åˆ†æ

| ä¼˜ç‚¹                                                         | ç¼ºç‚¹                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1. å……åˆ†åŒºåˆ†åæ¬¡æ¢¯åº¦ï¼› 2. å¯¹â€œç¼ºå¸­æ–‡æ¡£â€æœ‰å¹³æ»‘ç­–ç•¥ï¼Œé¿å… 0/1 æ–­å±‚ï¼› 3. å®ç°ä»ç„¶ç®€å•ï¼Œåªéœ€éå†ä¸€æ¬¡åˆ—è¡¨ã€‚ | 1. å¹³å‡åˆ†ç­–ç•¥è¾ƒä¸ºæ­¦æ–­ï¼Œå¯èƒ½é«˜ä¼°ä»æœªå‡ºç°è¿‡çš„æ–‡æ¡£ï¼› 2. å½“åˆ—è¡¨æ·±åº¦ä¸åŒã€c å¾ˆå¤§æ—¶ï¼Œæœªå‡ºç°æ–‡æ¡£å æ¯”é«˜ï¼Œå¹³æ»‘å™ªå£°å¤šï¼› 3. åŒæ ·é»˜è®¤ç³»ç»Ÿç­‰æƒï¼Œéœ€è¦å¤–éƒ¨è°ƒæƒï¼ˆWeighted Bordaï¼‰ã€‚ |

## 4 | Reciprocal Rank Fusion (RRF)

### 4.1 æ ¸å¿ƒå…¬å¼

ç»™å®šå‚æ•° **kï¼ˆé»˜è®¤ 60ï¼‰**ï¼š
$$
\operatorname{RRF}_{\text {score }}(d)=\sum_{r \in R} \frac{1}{k+\operatorname{rank}_r(d)}
$$

- è‹¥æ–‡æ¡£ d ä¸åœ¨ç»“æœ r ä¸­ï¼Œåˆ™è¯¥é¡¹è®° 0ï¼›æ­¤ä¸¾å¤©ç„¶æ”¯æŒ*Skimming*ï¼ˆrank è¶Šé å‰åˆ†æ¯è¶Šå°ï¼‰ã€‚
- ç”±äºåˆ†æ•°æ°¸è¿œ â‰¤ 1/kï¼Œå¤šä¸ªç»“æœåˆ—è¡¨ç´¯åŠ æ—¶ï¼Œä¸ä¼šå‡ºç°æç«¯çˆ†åˆ†ã€‚

### 4.2 å¿«é€Ÿä¼°ç®—

- rank=1 æ—¶å¾—åˆ† â‰ˆ 0.0164ï¼›rank=10 æ—¶ â‰ˆ 0.0143ï¼Œè¡°å‡éå¸¸ç¼“å’Œâ€”â€”**é¼“åŠ±å¬å›**ï¼ŒåŒæ—¶åˆä¸ä¼šè®©æ·±æ’æ–‡æ¡£ç¿»ç›˜ã€‚åˆ—è¡¨è¶Šå¤šï¼ŒåŒæ’ä½å¾—åˆ†å¯çº¿æ€§å åŠ ã€‚

### 4.3 ç¤ºä¾‹ & ç°è±¡

- ä¸¤ç³»ç»Ÿæ ·ä¾‹ä¸­ï¼Œd5/d14/d1 å‡å‡ºç°åœ¨åŒåˆ—è¡¨å‰ 7 ä½ï¼Œå› æ­¤ç´¯åŠ åç¨³å±…å‰ä¸‰ï¼š

  ```
  d5 (0.0325) > d14 (0.0315) > d1 (0.0303)
  ```

- **ç¨³å¥æ€§**ï¼šRRF å¯¹ç³»ç»Ÿå¤šå°‘ã€åˆ—è¡¨æ·±åº¦ã€rank æŠ–åŠ¨éƒ½ä¸æ•æ„Ÿï¼Œè¢«å¤§é‡å·¥ä¸šä¸å­¦æœ¯æ£€ç´¢ç®¡çº¿é‡‡çº³ï¼ˆå¦‚ TREC Deep Learning Task é‡Œçš„å®˜æ–¹ baselineï¼‰ã€‚

### 4.4 ä¼˜åŠ£å¯¹ç…§

| ä¼˜ç‚¹                                                         | ç¼ºç‚¹                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1. ä»…ä¾ rankï¼Œé›¶ä¾èµ–å½’ä¸€åŒ–ï¼› 2. å•è°ƒé€’å‡ã€ä¸Šç•Œæœ‰é™ï¼Œæ˜“äºè§£é‡Šï¼› 3. å®éªŒä¸­å¸¸ä¸å­¦ä¹ -to-rankæ——é¼“ç›¸å½“ï¼ˆç”šè‡³æ›´å¥½ï¼‰ã€‚ | 1. k å›ºå®š 60 è™½ç»éªŒæœ‰æ•ˆï¼Œä½†ä¸åŒåœºæ™¯è‹¥ä¸¥é‡ä¸åŒ¹é…åˆ—è¡¨é•¿åº¦ï¼Œä¹Ÿéœ€è°ƒå‚ï¼› 2. ç³»ç»Ÿç­‰æƒï¼›è‹¥å­˜åœ¨æ˜æ˜¾é«˜ä½è´¨é‡å·®å¼‚ï¼Œä»éœ€åŠ æƒæˆ–å…ˆæŒ‘ç³»ç»Ÿã€‚ |

## 5 | å…¶å®ƒ Rank-Based å˜ä½“

| æ–¹æ³•                      | å…³é”®æ€æƒ³                                          | é€‚ç”¨åœºæ™¯               |
| ------------------------- | ------------------------------------------------- | ---------------------- |
| **Weighted Interleaving** | å…ˆç”¨å†å²è¯„æµ‹ç»™å„ç³»ç»Ÿæ‰“æƒé‡ï¼Œè½®æ¬¡æŒ‰æƒé‡æŠ½æ ·        | A/B testã€åœ¨çº¿å­¦ä¹      |
| **Condorcet-Fuse**        | ç”¨â€œæˆå¯¹èƒœç‡â€æ›¿ä»£ Borda è®¡æ•°ï¼Œæ‰¾å‡ºç¾¤ä½“æ›´åå¥½çš„æ–‡æ¡£ | æŠ•ç¥¨ç†è®ºæ·±å…¥ã€ç³»ç»Ÿæ•°å¤š |
| **Weighted Borda**        | Borda åˆ†æ•°å†ä¹˜ç³»ç»Ÿæƒé‡                            | ç®€å•å‡çº§ï¼Œæ˜“è½åœ°       |

> è¿™äº›æ–¹æ³•çš„å…±åŒç‚¹ï¼š**å…ˆè·å–æˆ–ä¼°è®¡ç³»ç»Ÿæ€§èƒ½** â†’ **å°†å¥½ç³»ç»Ÿä¿¡å·æ”¾å¤§**ï¼Œä»¥æŠµå¾¡ Dark Horse Effectã€‚

# 09a Fusion: Score-Based

> **Score-Based Fusion** æŒ‡çš„æ˜¯ä½¿ç”¨å¤šä¸ªä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼ˆIR systemsï¼‰è¿”å›çš„æ–‡æ¡£â€œå¾—åˆ†â€æ¥è¿›è¡Œç»“æœåˆå¹¶ä¸æ’åºçš„èåˆæ–¹æ³•ã€‚

## CombSUM ç®—æ³•

ğŸ“Œ å®šä¹‰ï¼ˆDefinitionï¼‰

> **CombSUM** æ˜¯ Fox & Shaw åœ¨ 1994 å¹´æå‡ºçš„ä¸€ç§ç»å…¸çš„åˆ†æ•°èåˆæ–¹æ³•ã€‚

å®ƒçš„åšæ³•æ˜¯ï¼š

- **å¯¹æ¯ä¸ªæ–‡æ¡£åœ¨å„ä¸ª IR ç³»ç»Ÿä¸­çš„å¾—åˆ†è¿›è¡Œæ±‚å’Œ**ï¼Œä½œä¸ºæœ€ç»ˆçš„èåˆå¾—åˆ†ï¼ˆfused scoreï¼‰ã€‚

ğŸ¯ å…¬å¼ï¼š

ScoreCombSUM(d)=âˆ‘sâˆˆSscores(d)\text{Score}_{\text{CombSUM}}(d) = \sum_{s \in S} \text{score}_s(d)ScoreCombSUM(d)=âˆ‘sâˆˆSscores(d)

- SSSï¼šæ‰€æœ‰å‚ä¸èåˆçš„ç³»ç»Ÿé›†åˆ
- scores(d)\text{score}_s(d)scores(d)ï¼šç³»ç»Ÿ s ç»™æ–‡æ¡£ d çš„å¾—åˆ†

ğŸ“Œ ä¼˜ç‚¹ï¼š

- åˆ©ç”¨äº† **Skimming Effect**ï¼ˆé¡¶éƒ¨æ–‡æ¡£æ›´å¯èƒ½ç›¸å…³ï¼‰
- ä¹Ÿä½“ç°äº† **Chorus Effect**ï¼ˆå¤šç³»ç»Ÿä¸€è‡´è®¤ä¸ºç›¸å…³çš„æ–‡æ¡£å¾—åˆ†æ›´é«˜ï¼‰

âœ… ç¤ºä¾‹ï¼š

- System A: 0.45
- System B: 0.3
- System C: 0.35
- CombSUM Score = 1.1

åˆ†æ•°å½’ä¸€åŒ–ï¼ˆScore Normalisationï¼‰

ä¸åŒçš„ç³»ç»Ÿå¯èƒ½ç»™å‡ºçš„å¾—åˆ†**åˆ†å¸ƒèŒƒå›´ä¸åŒ**ï¼Œæ¯”å¦‚ï¼š

- System Aï¼šåˆ†æ•°åœ¨ 0 åˆ° 1000 ä¹‹é—´
- System Bï¼šåˆ†æ•°åœ¨ 0 åˆ° 1 ä¹‹é—´

è¿™ç§æƒ…å†µä¸‹ä¸èƒ½ç›´æ¥åŠ åˆ†æ•°ï¼Œéœ€è¦ç»Ÿä¸€æ ‡å‡†ã€‚

ğŸ”§ æ ‡å‡†å½’ä¸€åŒ–å…¬å¼ï¼š



å½’ä¸€åŒ–åï¼š

- æœ€ä½³æ’åæ–‡æ¡£å¾— 1
- æœ€å·®æ’åæ–‡æ¡£å¾— 0

å½’ä¸€åŒ–åæ‰è¿›è¡Œ CombSUMã€‚

## CombMNZ ç®—æ³•

å®šä¹‰ï¼š

**CombMNZï¼ˆMultiply by Non-Zeroï¼‰** æ˜¯ CombSUM çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºæ›´åŠ å¼ºåŒ– Chorus Effectã€‚

åšæ³•ï¼š

åœ¨ CombSUM çš„åŸºç¡€ä¸Šï¼Œ**ä¹˜ä»¥è¯¥æ–‡æ¡£å‡ºç°åœ¨å¤šå°‘ä¸ªç»“æœé›†ä¸­ï¼ˆå³éé›¶å¾—åˆ†æ¬¡æ•°ï¼‰**ã€‚

ç¤ºä¾‹ï¼š

- ç³»ç»Ÿ A: 0.45ï¼Œç³»ç»Ÿ B: 0.3ï¼Œç³»ç»Ÿ C: 0.35
- CombSUM = 1.1ï¼Œå‡ºç° 3 æ¬¡ â†’ CombMNZ = 3.3

## Linear Combination Modelï¼ˆçº¿æ€§åŠ æƒæ¨¡å‹ï¼‰

â— é—®é¢˜ï¼š

CombSUM/CombMNZ é»˜è®¤æ¯ä¸ªç³»ç»Ÿè´¨é‡ç›¸åŒã€‚ä½†ç°å®ä¸­æœ‰äº›ç³»ç»Ÿæœç´¢æ•ˆæœå¥½ï¼Œæœ‰äº›å·®ã€‚

ğŸ“Œ å®šä¹‰ï¼š

ä¸ºæ¯ä¸ªç³»ç»Ÿåˆ†é…ä¸€ä¸ª**æƒé‡ï¼ˆweightï¼‰**ï¼Œç„¶åå†åŠ æƒæ±‚å’Œã€‚

ScoreLinear(d)=âˆ‘sâˆˆSwsâ‹…normalised_scores(d)\text{Score}_{\text{Linear}}(d) = \sum_{s \in S} w_s \cdot \text{normalised\_score}_s(d)ScoreLinear(d)=sâˆˆSâˆ‘wsâ‹…normalised_scores(d)

âœ… ç¤ºä¾‹ï¼š

System A, B, C æƒé‡åˆ†åˆ«ä¸º 1, 2, 3

- Scores: 0.45, 0.3, 0.35
- æ€»å¾—åˆ† = 0.45Ã—1+0.3Ã—2+0.35Ã—3=2.10.45 $\times$ 1 + 0.3$\times$ 2 + 0.35 $\times$ 3 = 2.10.45Ã—1+0.3Ã—2+0.35Ã—3=2.1

å¦‚ä½•å†³å®šç³»ç»Ÿæƒé‡ï¼Ÿ

1. **CORI algorithm**ï¼šéœ€è¦äº†è§£æ¯ä¸ªç³»ç»Ÿæ‰€ä½¿ç”¨çš„æ–‡æ¡£é›†çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŸºäºæ–‡æ¡£é¢‘ç‡å’Œé›†åˆé¢‘ç‡ç­‰ä¿¡æ¯è®¡ç®—ã€‚
2. **LMS**ï¼ˆLength-based Merging Scoreï¼‰ï¼šä¾æ®ç»“æœé›†çš„é•¿åº¦æ¥å†³å®šæƒé‡ï¼Œç»“æœé›†è¶Šé•¿æƒé‡è¶Šå¤§ï¼ˆåœ¨å®è·µä¸­æ•ˆæœå±…ç„¶ä¸é”™ï¼‰ã€‚

# 09b Fusion-Probability-Based

**åŸºæœ¬åŠ¨æœºï¼šä¸ºä»€ä¹ˆè¦å¼•å…¥æ¦‚ç‡èåˆï¼ˆProbabilistic Fusionï¼‰**

ä¼ ç»Ÿ Score-Based èåˆæ–¹æ³•ï¼ˆå¦‚ CombSUMã€CombMNZï¼‰å­˜åœ¨ä¸¤ä¸ªé™åˆ¶ï¼š

1. **å¯¹æ‰€æœ‰ç³»ç»Ÿä¸€è§†åŒä»**ï¼Œå³å‡è®¾æ¯ä¸ªç³»ç»Ÿçš„è¡¨ç°ä¸€è‡´ã€‚
2. **åªçœ‹åˆ†æ•°ï¼Œä¸è€ƒè™‘ä½ç½®è¡¨ç°**ï¼Œå¿½è§†äº†æŸäº›ç³»ç»Ÿå¯èƒ½â€œå‰å‡ åæ–‡æ¡£è´¨é‡ç‰¹åˆ«é«˜â€è€Œåç»­è¾ƒå·®çš„ç‰¹ç‚¹ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

> ä½¿ç”¨å†å²è®­ç»ƒæ•°æ®æ¥å­¦ä¹ â€œæ¯ä¸ªç³»ç»Ÿåœ¨æ¯ä¸ªä½ç½®æ®µï¼ˆsegmentï¼‰è¿”å›æ–‡æ¡£ç›¸å…³æ€§çš„æ¦‚ç‡â€ï¼Œå¹¶æ®æ­¤æ‰“åˆ†ã€‚

## ProbFuse ç®—æ³•

- å°†æ¯ä¸ªç»“æœé›†åˆï¼ˆresult setï¼‰åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ª**ç­‰å¤§å°çš„æ®µï¼ˆsegmentsï¼‰**ï¼Œç¼–å·ä¸º k=1,2,â€¦,xã€‚

- å¯¹äºç³»ç»Ÿè¿”å›æ–‡æ¡£åœ¨ç¬¬ k æ®µçš„ä½ç½®ï¼Œé¢„å…ˆè®¡ç®—å®ƒâ€œè¢«ç›¸å…³â€çš„æ¦‚ç‡ã€‚

- æœ€ç»ˆæ‰“åˆ† = å„ç³»ç»Ÿå¯¹æ–‡æ¡£åˆ†é…çš„æ¦‚ç‡å€¼ä¹‹å’Œï¼ˆè€ƒè™‘ Skimming + Chorus Effectï¼‰

**ProbFuse çš„ä¸¤ä¸ªé˜¶æ®µ**

1. è®­ç»ƒé˜¶æ®µï¼ˆTraining Phaseï¼‰

åŸºäºä¸€ç»„æœ‰æ ‡æ³¨çš„æŸ¥è¯¢å†å²ï¼Œè®¡ç®—æ¯ä¸ªç³»ç»Ÿåœ¨æ¯ä¸ª segment ä¸­è¿”å›ç›¸å…³æ–‡æ¡£çš„æ¦‚ç‡ï¼š
$$
P_k=\frac{\text { segment } k \text { ä¸­ç›¸å…³æ–‡æ¡£æ•°é‡ }}{\text { segment } k \text { çš„æ€»æ–‡æ¡£æ•° }} \text {, å¯¹æ‰€æœ‰è®­ç»ƒæŸ¥è¯¢æ±‚å¹³å‡ }
$$
ç¤ºä¾‹ï¼ˆx = 4 æ®µï¼‰ï¼š

| Segment k | è®­ç»ƒæŸ¥è¯¢1 | è®­ç»ƒæŸ¥è¯¢2 | è®­ç»ƒæŸ¥è¯¢3 | å¹³å‡æ¦‚ç‡ |
| --------- | --------- | --------- | --------- | -------- |
| k=1       | 3/3       | 2/3       | 1/3       | 0.67     |
| k=2       | 2/3       | 1/3       | 1/3       | 0.44     |
| k=3       | 1/3       | 1/3       | 0/3       | 0.22     |
| k=4       | 0/3       | 0/3       | 1/3       | 0.11     |



------

2. èåˆé˜¶æ®µï¼ˆFusion Phaseï¼‰

å¯¹äºæ¯æ¡æ–‡æ¡£ï¼š

- æŸ¥çœ‹å®ƒåœ¨æ¯ä¸ªç³»ç»Ÿä¸­çš„ä½ç½®ï¼Œå±äºç¬¬ k æ®µ
- è·å–è®­ç»ƒé˜¶æ®µä¸­è¯¥ç³»ç»Ÿè¯¥æ®µå¯¹åº”çš„æ¦‚ç‡ $P_k$
- ä¸ºäº†åˆ©ç”¨ Skimming Effectï¼Œåˆ†æ•° = $\frac{P_k}{k}$
- å¯¹æ‰€æœ‰ç³»ç»Ÿæ±‚å’Œï¼Œä½œä¸ºæœ€ç»ˆå¾—åˆ†

$$
\operatorname{Final} \operatorname{Score}(d)=\sum_{\text {systems }} \frac{P_k}{k}
$$

## SegFuse ç®—æ³•ï¼ˆæ”¹è¿›1ï¼šå¯å˜æ®µå®½ï¼‰

**åŠ¨æœº**

ProbFuse çš„å›ºå®šåˆ†æ®µå¿½ç•¥äº†â€œå‰å‡ ä¸ªæ–‡æ¡£æå¯èƒ½ç›¸å…³â€çš„ç°è±¡ã€‚ä¾‹å¦‚ï¼š

- ç¬¬1æ®µ40ç¯‡æ–‡æ¡£ä¸­ï¼Œæ’ç¬¬1ä½å’Œæ’ç¬¬40ä½çš„æ–‡æ¡£è¢«ä¸€è§†åŒä»ã€‚

**SegFuse æ ¸å¿ƒæ€æƒ³ï¼š**

- **ä½¿ç”¨éå‡ç­‰å®½åº¦çš„æ®µï¼ˆæ®µæ•°å°‘ï¼Œè¶Šå¾€åè¶Šå®½ï¼‰**
- æ¯æ®µå®½åº¦ï¼šSizek=10Ã—2kâˆ’1âˆ’5\text{Size}_k = 10 \times 2^{k-1} - 5Sizek=10Ã—2kâˆ’1âˆ’5
  - k=1 â†’ 5
  - k=2 â†’ 15
  - k=3 â†’ 35
  - k=4 â†’ 75
  - â€¦

æ‰“åˆ†æ–¹å¼ï¼š

- æ¯ä¸ªæ–‡æ¡£æ‰€åœ¨æ®µçš„æ¦‚ç‡ Ã— æ–‡æ¡£çš„æ ‡å‡†åŒ–ä½ç½®åˆ†æ•°ï¼ˆnormalised scoreï¼‰

è¿™æ¯” ProbFuse çš„ Pkk\frac{P_k}{k}kPk æ›´ç»†ç²’åº¦åœ°å¼ºè°ƒäº†**ä½ç½®ä¿¡æ¯çš„é‡è¦æ€§**ã€‚

## SlideFuse ç®—æ³•ï¼ˆæ”¹è¿›2ï¼šæ»‘åŠ¨çª—å£ï¼‰

**åŠ¨æœº**

SegFuse è™½æ”¹è¿›äº†æ®µå®½ï¼Œä½†ä»ç„¶å­˜åœ¨â€œç›¸é‚»æ–‡æ¡£åˆ†æ•°å·®å¼‚å·¨å¤§â€çš„é—®é¢˜ã€‚

**ä¾‹å­ï¼š**

- æ–‡æ¡£40 ä¸æ–‡æ¡£41 å±äºä¸åŒæ®µï¼Œåˆ†æ•°æ–­å´–å¼ä¸‹é™ã€‚

**SlideFuse æ ¸å¿ƒæ€æƒ³ï¼š**

- ä¸ä½¿ç”¨åˆ†æ®µï¼Œè€Œæ˜¯**æ¯ä¸ªæ–‡æ¡£çš„æ‰“åˆ†ç”±å…¶ä½ç½®å‘¨å›´çš„æ»‘åŠ¨çª—å£ï¼ˆsliding windowï¼‰è®¡ç®—å¾—å‡ºæ¦‚ç‡**ã€‚
- å¹³æ»‘æ‰é”¯é½¿çŠ¶çš„åˆ†å¸ƒï¼Œä½¿ç›¸é‚»ä½ç½®å¾—åˆ†æ›´åˆç†ã€‚

**å®ç°ï¼š**

- åœ¨è®­ç»ƒé˜¶æ®µè®¡ç®—æ¯ä¸ª rank çš„ç›¸å…³æ¦‚ç‡
- åœ¨èåˆé˜¶æ®µï¼šæ¯ä¸ªæ–‡æ¡£ä½¿ç”¨è‡ªå·± rank å‘¨å›´çš„æ»‘åŠ¨çª—å£çš„æ¦‚ç‡å¹³å‡å€¼

# 10-PageRank

## 1. Introduction

ä¼ ç»Ÿçš„æ’åæ¨¡å‹ï¼ˆå¦‚ Vector Spaceã€BM25ï¼‰ä»…ä»…åŸºäº**å•ä¸ªæ–‡æ¡£çš„å†…å®¹**è¿›è¡Œè®¡ç®—ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ã€‚

> è¿™ä¸äººç±»åˆ¤æ–­æ–‡æ¡£é‡è¦æ€§çš„æ–¹æ³•ä¸åŒï¼šäººç±»ä¼šå‚è€ƒå…¶ä»–æ–‡æ¡£å¯¹å®ƒçš„å¼•ç”¨ã€‚

ä¸ºæ­¤ï¼Œ**Learning to Rank** æå‡ºå¼•å…¥å…¶å®ƒâ€œç‰¹å¾â€ï¼Œå¦‚æ–‡æ¡£çš„â€œé‡è¦æ€§â€ï¼ˆimportanceï¼‰ã€‚PageRank å°±æ˜¯è¡¡é‡ç½‘é¡µé‡è¦æ€§çš„ä¸€ç§æ–¹å¼ã€‚

## 2. æ–‡æ¡£é‡è¦æ€§ä¸å¼•ç”¨ï¼ˆDocument Importanceï¼‰

å­¦æœ¯ä¸­é€šå¸¸ç”¨è¢«å¼•ç”¨æ¬¡æ•°ï¼ˆcitation countï¼‰æ¥è¡¡é‡è®ºæ–‡çš„å½±å“åŠ›ã€‚ç±»ä¼¼çš„æ€æƒ³ä¹Ÿé€‚ç”¨äºç½‘é¡µï¼š

- ç½‘é¡µä¹‹é—´å­˜åœ¨è¶…é“¾æ¥ï¼ˆhyperlinksï¼‰ï¼›
- å¦‚æœæŸä¸ªç½‘é¡µè¢«å¾ˆå¤šå…¶ä»–ç½‘é¡µé“¾æ¥ï¼ˆbacklinkï¼‰ï¼Œåˆ™è¯´æ˜å®ƒå¾ˆâ€œé‡è¦â€ã€‚

## 3. ç½‘é¡µä¸å¼•ç”¨è®¡æ•°çš„å¼‚åŒ

- ğŸ“š å­¦æœ¯å¼•ç”¨æ˜¯**å•å‘çš„**ï¼šAåªèƒ½å¼•ç”¨å·²å‘è¡¨çš„Bï¼›
- ğŸŒ ç½‘é¡µå¼•ç”¨å¯èƒ½æ˜¯**åŒå‘çš„**ï¼ˆå¾ªç¯å¼•ç”¨/circular referencesï¼‰ï¼›
- å­¦æœ¯å¼•ç”¨æœ‰è´¨é‡å®¡æ ¸ï¼ˆpeer reviewï¼‰ï¼Œç½‘é¡µæ²¡æœ‰ï¼›
- å› æ­¤ï¼Œç½‘ç»œä¸­çš„é“¾æ¥æ›´å®¹æ˜“è¢«â€œä½œå¼Šâ€ï¼ˆlink farmsç­‰ï¼‰ã€‚

## 4. PageRank çš„æ ¸å¿ƒæ€æƒ³

ç”±è°·æ­Œåˆ›å§‹äºº Sergey Brin å’Œ Larry Page åœ¨ 1998 å¹´æå‡ºã€‚

åŸºæœ¬å…¬å¼å¦‚ä¸‹ï¼š
$$
R(u)=\sum_{v \in B_u} \frac{R(v)}{N_v}
$$
$R(u)$ ï¼šé¡µé¢uçš„PageRankï¼›
$B_u$ ï¼šæ‰€æœ‰æŒ‡å‘ u çš„é¡µé¢é›†åˆï¼ˆbacklinksï¼‰ï¼›
$R(v)$ ï¼šé¡µé¢ v çš„PageRankï¼›
$N_v$ ï¼šé¡µé¢vçš„å‡ºé“¾æ•°é‡ï¼ˆoutlinksï¼‰ã€‚

è§£é‡Šï¼šé¡µé¢vå°†è‡ªå·±çš„PageRankå¹³å‡åˆ†ç»™å®ƒæŒ‡å‘çš„æ¯ä¸€ä¸ªé¡µé¢ã€‚

**åˆå§‹å€¼ä¸è¿­ä»£**

- åˆå§‹æ—¶ç»™æ‰€æœ‰é¡µé¢ä¸€ä¸ªç›¸åŒçš„PageRankï¼ˆå¦‚1ï¼‰ï¼›
- æ¯è½®è¿­ä»£æ ¹æ®å…¬å¼è®¡ç®—æ–°å€¼ï¼Œç›´åˆ°æ”¶æ•›ï¼ˆæ•°å€¼å˜åŒ–å¾ˆå°ï¼‰ï¼›
- Brin & Pageå®éªŒä¸­ï¼Œæ•°æ®é‡ä¸º3.22äº¿é“¾æ¥ï¼Œ52æ¬¡è¿­ä»£æ”¶æ•›ã€‚

## 5. Rank Sink é—®é¢˜ä¸ Damping Factor é˜»å°¼å› å­

**é—®é¢˜**ï¼šå¦‚æœè‹¥å¹²é¡µé¢åªäº’ç›¸é“¾æ¥ï¼Œä¸æŒ‡å‘å¤–éƒ¨ï¼Œå®ƒä»¬çš„PageRankä¼šä¸æ–­å¢é•¿ï¼Œå½¢æˆâ€œRank Sinkâ€ã€‚

**è§£å†³æ–¹æ³•**ï¼šå¼•å…¥é˜»å°¼å› å­ dddï¼Œé€šå¸¸å–å€¼ä¸º0.85ï¼š
$$
R(u)=(1-d)+d \cdot \sum_{v \in B_u} \frac{R(v)}{N_v}
$$

- $1âˆ’d$ï¼šè¡¨ç¤ºâ€œéšæœºè·³è½¬â€åˆ°ä»»ä¸€é¡µé¢çš„æ¦‚ç‡ï¼›
- ä¿è¯æ‰€æœ‰é¡µé¢éƒ½æœ‰æœ€å°PageRankï¼Œä¸è‡³äºä¸ºé›¶ã€‚

## 6. æ²¡æœ‰Backlinkçš„é¡µé¢æ€ä¹ˆåŠ

æ—§å…¬å¼ä¸‹ï¼Œæ²¡æœ‰backlinkçš„é¡µé¢PageRankä¸º0ï¼Œä¸”ä¸ä¼šä¼ é€’ä»»ä½•æƒé‡ã€‚
 æ–°å…¬å¼é€šè¿‡ 1âˆ’d1-d1âˆ’d ä¿è¯å³ä½¿æ²¡æœ‰å¼•ç”¨ä¹Ÿæœ‰æœ€ä½PageRankå€¼ã€‚

## 7. PageRank çš„å®é™…æ„ä¹‰ä¸å½±å“

- PageRank æˆä¸º Google æœç´¢æˆåŠŸçš„å…³é”®å› ç´ ä¹‹ä¸€ï¼›
- ç°ä»£æœç´¢å¼•æ“èåˆäº†å¤šä¸ªæ’åºä¿¡å·ï¼ˆå¦‚å…¨æ–‡åŒ¹é…ã€æ ‡é¢˜ã€è¿‘ä¼¼åº¦ç­‰ï¼‰ï¼›
- PageRank ä»è¢«ç”¨äºæœç´¢æ’åºçš„ä¸€éƒ¨åˆ†ï¼ˆä½†å®ç°ç»†èŠ‚ä¿å¯†ï¼‰ã€‚

# 11a-Relevance Feedback

## 1. æ¦‚å¿µï¼š Relevance Feedbackï¼ˆç›¸å…³åé¦ˆï¼‰

**Relevance Feedback** æ˜¯æŒ‡ï¼šç”¨æˆ·å‚ä¸åˆ°ä¿¡æ¯æ£€ç´¢è¿‡ç¨‹ï¼Œé€šè¿‡å¯¹åˆå§‹è¿”å›ç»“æœä¸­å“ªäº›æ˜¯â€œç›¸å…³â€ï¼ˆrelevantï¼‰ã€å“ªäº›æ˜¯â€œä¸ç›¸å…³â€ï¼ˆnon-relevantï¼‰çš„æ–‡æ¡£è¿›è¡Œæ ‡æ³¨ï¼Œä»è€Œå¸®åŠ©ç³»ç»Ÿâ€œé‡æ–°ç†è§£â€ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œå¹¶è¿›ä¸€æ­¥ç”Ÿæˆæ›´ä¼˜çš„ç»“æœã€‚

- é€šå¸¸ç”¨äºæé«˜æ£€ç´¢ç²¾åº¦ï¼ˆprecisionï¼‰ï¼Œé€šè¿‡ç”¨æˆ·åé¦ˆè°ƒæ•´æŸ¥è¯¢ã€‚

## 2. RFè¿‡ç¨‹

- User **provides a query** 
- The system returns an **initial set of results**
- The user marks some **relevant** and **nonrelevant** 
- The system computes a **better representation of the information need** based on this feedback
- The system returns a **revised set of results**
- Possibly the above process is repeated.

## 3. Rocchio Algorithmï¼šæœ€è‘—åçš„åé¦ˆç®—æ³•

**èƒŒæ™¯**

- Rocchio ç®—æ³•æ˜¯**çŸ¢é‡ç©ºé—´æ¨¡å‹ï¼ˆVector Space Modelï¼‰**çš„ä¸€éƒ¨åˆ†ã€‚

- ç›®æ ‡æ˜¯ï¼šä½¿ä¿®æ”¹åçš„æŸ¥è¯¢å‘é‡**é è¿‘ç›¸å…³æ–‡æ¡£çš„å‘é‡ä¸­å¿ƒ**ï¼ˆcentroidï¼‰ï¼Œè¿œç¦»éç›¸å…³æ–‡æ¡£çš„å‘é‡ã€‚

**ç†è®ºå…¬å¼ï¼š**
$$
\vec{q}_{\text {new }}=\alpha \vec{q}_{\text {original }}+\frac{\beta}{\left|D_r\right|} \sum_{\vec{d}_r \in D_r} \vec{d}_r-\frac{\gamma}{\left|D_{n r}\right|} \sum_{\vec{d}_{n r} \in D_{n r}} \vec{d}_{n r}
$$

## 4. Assumptions of Relevance Feedbackï¼ˆå‰æå‡è®¾ï¼‰

ç³»ç»Ÿå‡è®¾ä»¥ä¸‹å‡ ç‚¹æˆç«‹ï¼š

1. **ç”¨æˆ·æŸ¥è¯¢éœ€è¶³å¤Ÿæ¥è¿‘ç›¸å…³æ–‡æ¡£**ï¼šå¦åˆ™ç³»ç»Ÿæ— æ³•åˆæ¬¡è¿”å›ç›¸å…³æ–‡æ¡£ä¾›ç”¨æˆ·æ ‡æ³¨ã€‚
2. **ç›¸å…³æ–‡æ¡£ä¹‹é—´è¯­ä¹‰ç›¸ä¼¼ã€èšç±»é è¿‘**ï¼šä½†ç°å®ä¸­æœ‰æ—¶ä¸æˆç«‹ï¼Œä¾‹å¦‚â€œå®‡èˆªå‘˜ vs. å®‡èˆªäººâ€ï¼ˆastronaut vs. cosmonautï¼‰ã€‚
3. ä¸èƒ½è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
   - æ‹¼å†™é”™è¯¯ï¼ˆmisspellingsï¼‰
   - è·¨è¯­è¨€æ£€ç´¢ï¼ˆcross-language IRï¼‰
   - è¯æ±‡ä¸åŒ¹é…ï¼ˆvocabulary mismatchï¼‰

## 5. Pseudo Relevance Feedbackï¼ˆä¼ªç›¸å…³åé¦ˆï¼‰

ç°å®ä¸­å¾ˆå°‘æœ‰ç”¨æˆ·æ„¿æ„æ˜ç¡®åé¦ˆï¼Œäºæ˜¯æå‡ºäº† **Pseudo Relevance Feedbackï¼ˆPRFï¼‰**ï¼Œä¹Ÿç§°ä¸º **Blind Relevance Feedback**ï¼š

- å‡è®¾è¿”å›ç»“æœå‰ kkk ä¸ªæ–‡æ¡£æ˜¯ç›¸å…³çš„ï¼›
- ç„¶åç”¨è¿™äº›æ–‡æ¡£ä½œä¸ºâ€œç›¸å…³æ–‡æ¡£â€æ‰§è¡Œ Rocchio ç®—æ³•ï¼›
- é€šå¸¸æœ‰æ•ˆï¼Œä½†åœ¨æŸäº›æŸ¥è¯¢ä¸­ä¹Ÿå¯èƒ½å¤±è´¥ã€‚

## 6. Implicit Relevance Feedbackï¼ˆéšå¼åé¦ˆï¼‰

é™¤äº†ç”¨æˆ·æ˜¾å¼æ ‡æ³¨å¤–ï¼Œç³»ç»Ÿä¹Ÿå¯ä»¥é€šè¿‡ç”¨æˆ·è¡Œä¸ºæ¨æ–­å‡ºç›¸å…³æ€§ï¼š

- ç”¨æˆ·ç‚¹å‡»äº†å“ªäº›æ–‡æ¡£ï¼Ÿ
- åœç•™æ—¶é—´æ˜¯å¦è¾ƒé•¿ï¼Ÿ
- æ˜¯å¦å¿«é€Ÿè·³å‡ºï¼Ÿ

## 7. æ€»ç»“ï¼ˆSummaryï¼‰

| ç±»å‹                          | æ˜¯å¦ä¾èµ–ç”¨æˆ·å‚ä¸            | ä¼˜ç¼ºç‚¹è¯´æ˜               |
| ----------------------------- | --------------------------- | ------------------------ |
| **Relevance Feedback**        | æ˜¯                          | ç²¾åº¦é«˜ï¼Œä½†ä¸æ˜“æ¨å¹¿       |
| **Pseudo Relevance Feedback** | å¦ï¼Œè‡ªåŠ¨å‡è®¾å‰ k ä¸ªæ–‡æ¡£ç›¸å…³ | å®ç”¨ä½†æœ‰é£é™©             |
| **Implicit Feedback**         | å¦ï¼ŒåŸºäºç”¨æˆ·è¡Œä¸º            | ä¸éœ€æ‰“æ‰°ç”¨æˆ·ï¼Œéœ€å¤æ‚å»ºæ¨¡ |

# 11b-Query-Expansion

##1. ä»€ä¹ˆæ˜¯æŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰

> **Query Expansion** æ˜¯æŒ‡é€šè¿‡ä¸ºç”¨æˆ·æŸ¥è¯¢å¢åŠ ç›¸å…³æœ¯è¯­æ¥æ”¹å–„æ£€ç´¢æ€§èƒ½çš„ä¸€ç§æ–¹æ³•ã€‚

- **ç›®æ ‡**ï¼šå‡å°‘æŸ¥è¯¢ä¸æ–‡æ¡£ä¹‹é—´çš„â€œè¯­ä¹‰ä¸åŒ¹é…â€ï¼Œæå‡å¬å›ç‡ï¼ˆRecallï¼‰ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿå¯èƒ½æå‡å‡†ç¡®ç‡ï¼ˆPrecisionï¼‰ã€‚
- **æ–¹å¼**ï¼š
  - **è‡ªåŠ¨æŸ¥è¯¢æ‰©å±•ï¼ˆAutomatic QEï¼‰**
  - **äº¤äº’å¼æŸ¥è¯¢æ‰©å±•ï¼ˆInteractive QEï¼‰**ï¼šç”±ç”¨æˆ·å‚ä¸ï¼Œå¦‚ Google æç¤ºå…³é”®è¯è”æƒ³ã€‚

## 2. Resources for Query Expansion

**åŒä¹‰è¯åº“ï¼ˆThesaurusï¼‰**

- **äººå·¥æ„å»ºçš„è¯åº“**ï¼ˆå¦‚åŒ»å­¦/æ³•å¾‹é¢†åŸŸï¼‰ï¼š
  - ä¼˜ç‚¹ï¼šå‡†ç¡®åº¦é«˜
  - ç¼ºç‚¹ï¼šæˆæœ¬é«˜ï¼Œç»´æŠ¤å›°éš¾
- **è‡ªåŠ¨ç”Ÿæˆçš„åŒä¹‰è¯åº“**ï¼š
  - åˆ©ç”¨è¯é¢‘å…±ç°æˆ–è¯­æ³•å…³ç³»åˆ†æè‡ªåŠ¨æ„å»ºã€‚
  - ä¸¤ç§å®šä¹‰æ–¹å¼ï¼š
    1. å…±ç°ï¼ˆco-occurrenceï¼‰ç›¸ä¼¼æ€§ï¼ˆå¦‚â€œcarâ€å’Œâ€œtruckâ€ï¼‰
    2. è¯­æ³•è§’è‰²ç›¸ä¼¼æ€§ï¼ˆå¦‚â€œappleâ€å’Œâ€œpearâ€éƒ½å¯ä»¥è¢«â€œeatâ€ã€â€œpeelâ€ç­‰åŠ¨è¯ä½œç”¨ï¼‰

**æŸ¥è¯¢æ—¥å¿—æŒ–æ˜ï¼ˆQuery Log Miningï¼‰**

- åˆ©ç”¨å†å²ç”¨æˆ·æŸ¥è¯¢è¡Œä¸ºï¼š
  - å¦‚æœè®¸å¤šç”¨æˆ·åœ¨æœç´¢â€œfootballâ€ä¹‹åæœç´¢â€œsoccer ballâ€ï¼Œåˆ™è¿™ä¸¤ä¸ªè¯å¯èƒ½æ˜¯å½¼æ­¤çš„æ‰©å±•ã€‚
  - ç‚¹å‡»ç›¸åŒç»“æœé¡µé¢çš„è¯ç»„ä¹‹é—´ä¹Ÿå¯è®¤ä¸ºå…·æœ‰æ‰©å±•å…³ç³»ã€‚

##4. è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰

> è¯åµŒå…¥å°†æ¯ä¸ªå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ï¼ŒåŸºäºè·ç¦»æˆ–è§’åº¦æ¥è¯„ä¼°ç›¸ä¼¼åº¦ã€‚

- å¸¸è§æ¨¡å‹ï¼š
  - **word2vec**ï¼ˆæœ€ç»å…¸ï¼Œ300ç»´ï¼‰
  - **GloVe**
  - **ELMo**
  - **BERT Embeddings**ï¼ˆä»é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æå–ï¼‰
- **ä¼˜ç‚¹**ï¼šå¯è¡¡é‡å•è¯é—´çš„è¯­ä¹‰è·ç¦»ï¼Œä¸éœ€è¦ä¼ ç»Ÿè¯å…¸ã€‚

## 5. æŸ¥è¯¢æ‰©å±•çš„é£é™©ä¸æŒ‘æˆ˜

- è™½ç„¶å¯ä»¥å¢åŠ å¬å›ç‡ï¼ˆRecallï¼‰ï¼Œä½†å®¹æ˜“é™ä½å‡†ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š
  - ä¾‹å¦‚å°† â€œinterest rateâ€ æ‰©å±•ä¸º â€œinterest fascinate rateâ€ ä¼šå¼•å…¥æ— å…³è¯ã€‚

# 12a-Web Crawling

## 1. Introduction

- Many techniques IR were originally developed before the WWW became popular
- Nowadays, WWW is the biggest source and become an extremely important

## 2. ä»€ä¹ˆæ˜¯ Web Crawlerï¼Ÿ

Web Crawlerï¼ˆç½‘ç»œçˆ¬è™«ï¼‰ï¼Œåˆç§° Web Spiderï¼Œæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–ç¨‹åºï¼Œç”¨äºåœ¨ç½‘ä¸Šå¯»æ‰¾æ–‡æ¡£å¹¶å°†å…¶æ”¶å½•åˆ°æœç´¢å¼•æ“çš„ç´¢å¼•ä¸­ï¼Œä¾›ç”¨æˆ·æŸ¥è¯¢ä½¿ç”¨ã€‚

- å®ƒçš„å·¥ä½œæ–¹å¼æ˜¯è®¿é—®ç½‘é¡µï¼Œæå–å…¶ä¸­çš„ hyperlinksï¼ˆè¶…é“¾æ¥ï¼‰ï¼Œç„¶åç»§ç»­è®¿é—®è¿™äº›é“¾æ¥æŒ‡å‘çš„æ–°ç½‘é¡µã€‚
- é€šè¿‡è¿™ç§æ–¹å¼æ„å»ºä¸€ä¸ªå¯æœç´¢çš„ç½‘é¡µé›†åˆã€‚

**æœç´¢ç­–ç•¥ï¼ˆCrawling Strategyï¼‰**

- **Breadth-First Searchï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼‰** æ˜¯ä¸€ç§è¾ƒä¸ºå¸¸ç”¨çš„ç­–ç•¥ï¼š
  - è¢«è®¤ä¸ºå¯ä»¥æ›´å¥½åœ°å‘ç°é«˜è´¨é‡é¡µé¢ï¼ˆæŒ‰ PageRank è¡¡é‡ï¼‰ã€‚
  - æ›´åŠ â€œç¤¼è²Œâ€ï¼ˆpoliteï¼‰ï¼Œå› ä¸ºå®ƒä¸ä¼šå¯¹åŒä¸€æœåŠ¡å™¨é€ æˆè¿‡é«˜å‹åŠ›ã€‚
- **Depth-First Searchï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼‰** åˆ™ä½¿ç”¨å †æ ˆç»“æ„è€Œéé˜Ÿåˆ—ï¼Œè¡Œä¸ºæ›´åƒé€’å½’å¼æŠ“å–ã€‚

å¤§å¤šæ•°çˆ¬è™«å¹¶è¡Œè¿è¡Œï¼ˆparallel crawlingï¼‰ï¼Œä¸ä¼šå› ç­‰å¾…å“åº”è€Œæµªè´¹æ—¶é—´ã€‚

## 3. ç¤¼è²Œçˆ¬è™«ï¼ˆPolite Crawlersï¼‰

ä¸ºäº†é¿å…å¯¹ç½‘ç«™é€ æˆç ´åï¼Œçˆ¬è™«åº”éµå®ˆä¸€å®šçš„â€œç½‘ç»œç¤¼ä»ªâ€ã€‚

- **éµå®ˆ robots.txt æ–‡ä»¶**ï¼ˆRobots Exclusion Standardï¼‰
- Not overloading the website (obey Crawl-Delay).
- Not consuming too much bandwidth.
- Identifying who they are and who they belong to (â€œGooglebotâ€, â€œBaiduspiderâ€).

## 4. ä½¿ç”¨ `<meta>` æ ‡ç­¾æ§åˆ¶çˆ¬è™«è¡Œä¸º

- **noindex**ï¼šä¸å…è®¸è¯¥é¡µé¢è¢«æ”¶å½•

  ```
  <meta name="robots" content="noindex">
  ```

- **nofollow**ï¼šä¸å…è®¸è¯¥é¡µé¢ä¸Šçš„é“¾æ¥è¢«è¿½è¸ª

  ```
  <meta name="robots" content="nofollow">
  ```

- å•ä¸ªé“¾æ¥ä¹Ÿå¯ä»¥è®¾ç½® `rel="nofollow"`

## 5. ç½‘é¡µç»“æ„çš„æŒ‘æˆ˜ï¼ˆWeb-Specific Challengesï¼‰

**URL é‡å®šå‘å’Œé‡å¤é—®é¢˜**

**æœç´¢å¼•æ“ä¼˜åŒ–ï¼ˆSEO-friendly URLsï¼‰**

## 6. éé™æ€é¡µé¢ï¼ˆNon-static Documentsï¼‰

ç½‘é¡µå†…å®¹å˜åŒ–é¢‘ç¹ï¼Œå°¤å…¶æ˜¯æ–°é—»ç½‘ç«™å’Œç¤¾äº¤åª’ä½“å¹³å°ï¼š

- éœ€è¦ **å®šæœŸé‡æ–°æŠ“å–ï¼ˆre-crawlingï¼‰**ï¼Œé˜²æ­¢ç´¢å¼•è¿‡æ—¶ã€‚
- ä½†éƒ¨åˆ†å†…å®¹é™æ€ä¸”å˜åŒ–å°ï¼ˆå¦‚æ¡£æ¡ˆé¡µé¢ï¼‰å¯ä»¥å‡å°‘æ›´æ–°é¢‘ç‡ã€‚

å› æ­¤ï¼Œéœ€åˆ¶å®šä¸€ä¸ªç­–ç•¥æ¥åˆ¤æ–­ï¼š

- å“ªäº›é¡µé¢é¢‘ç¹æ›´æ–° â†’ é«˜é¢‘çˆ¬å–
- å“ªäº›é¡µé¢åŸºæœ¬ä¸å˜ â†’ ä½é¢‘æˆ–ä¸æ›´æ–°

## 7. å¯¹æŠ—çˆ¬è™«çš„æŠ€æœ¯ï¼ˆAnti-crawling Techniquesï¼‰

ä¾‹å¦‚ CAPTCHAï¼ˆå›¾å½¢éªŒè¯ç ï¼‰æœºåˆ¶ï¼š

- CAPTCHAï¼ˆCompletely Automated Public Turing test to tell Computers and Humans Apartï¼‰é˜»æ­¢è‡ªåŠ¨ç¨‹åºè®¿é—®ã€‚â†³
- è‡ªåŠ¨çˆ¬è™«æ— æ³•è½»æ¾è¯†åˆ«å›¾å½¢ä¸­çš„æ–‡å­—ï¼Œå› æ­¤æ— æ³•è¿›å…¥è¿™äº›é¡µé¢ã€‚

## 8. ç½‘é¡µçš„è§„æ¨¡ï¼ˆHow Big is the Web?ï¼‰

- Google æŒ‡å‡ºå…¶ç´¢å¼•åŒ…å«æ•°åƒäº¿ç½‘é¡µã€‚
- å®é™…ä¸Šç”±äºåŠ¨æ€ç½‘é¡µçš„å­˜åœ¨ï¼ŒWeb çš„è§„æ¨¡å¯ä»¥è§†ä¸ºâ€œæ— é™å¤§â€ã€‚

**Deep Webï¼ˆæ·±ç½‘ï¼‰æˆ– Hidden Webï¼ˆéšè—ç½‘ï¼‰**

- æ²¡æœ‰ä»»ä½•é“¾æ¥æŒ‡å‘çš„é¡µé¢éš¾ä»¥è¢«çˆ¬è™«å‘ç°ã€‚
- AJAX åŠ¨æ€ç”Ÿæˆçš„å†…å®¹æ— æ³•é€šè¿‡ä¼ ç»Ÿæ–¹å¼æŠ“å–ã€‚

## 12b-Adversarial Information Retrieval

### 1. Introduction

ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰åœºæ™¯ä¸­ï¼Œæ–‡æ¡£ä½œè€…å¹¶ä¸åˆ»æ„è€ƒè™‘æ–‡æ¡£å¦‚ä½•è¢«æœç´¢å¼•æ“å‘ç°å’Œæ’åï¼›è€Œåœ¨ Web Search ä¸­ï¼Œç½‘é¡µå‘å¸ƒè€…å¸¸å¸¸**åˆ»æ„å½±å“**å…¶åœ¨æœç´¢ç»“æœä¸­çš„æ’åã€‚

è¿™å¯¼è‡´äº†ä¸€ä¸ªæ–°é—®é¢˜çš„å‡ºç°ï¼š**Adversarial IR**ï¼Œå³ï¼šæŸäº›ç«™ç‚¹**æ•…æ„æ“çºµæœç´¢æ’å**ä»¥è·å–å•†ä¸šåˆ©ç›Šã€‚

**å…³é”®æœ¯è¯­ï¼š**

- **Search Engine Optimisation (SEO)**ï¼šæœç´¢å¼•æ“ä¼˜åŒ–
  - **White Hat SEO**ï¼šéµå®ˆè§„åˆ™ï¼Œåˆç†ä¼˜åŒ–
  - **Black Hat SEO**ï¼šè¿åè§„åˆ™ï¼Œæ¬ºéª—æœç´¢å¼•æ“ï¼Œå³â€œå¯¹æŠ—æ€§ IRâ€

## 2. å¸¸è§çš„ Adversarial IR æŠ€æœ¯

1. Meta Tags æ»¥ç”¨ï¼ˆå…ƒæ ‡ç­¾ï¼‰

HTML æä¾› `<meta>` æ ‡ç­¾è®©ä½œè€…æè¿°ç½‘é¡µå†…å®¹ï¼Œå¦‚å…³é”®è¯ï¼ˆkeywordsï¼‰å’Œç®€ä»‹ï¼ˆdescriptionï¼‰ã€‚

**é—®é¢˜**ï¼š

- æ—©æœŸæœç´¢å¼•æ“ä¸¥é‡ä¾èµ–è¿™äº›æ ‡ç­¾
- å¯¼è‡´ç«™ç‚¹æ•…æ„å¡«å…¥çƒ­é—¨å…³é”®è¯ï¼ˆå¦‚ "free"ï¼‰æ¥æ“çºµæ’å
- **ç°ä»£æœç´¢å¼•æ“å¤§å¤šå¿½ç•¥ `<meta keywords>`ï¼Œä»…éƒ¨åˆ†ä½¿ç”¨ `<meta description>`**

2. Hidden Textï¼ˆéšè—æ–‡æœ¬ï¼‰

é€šè¿‡è®¾ç½®å­—ä½“é¢œè‰²ä¸èƒŒæ™¯ä¸€è‡´ï¼ŒåŠ å…¥**ä¸å¯è§å…³é”®è¯**ï¼Œä»¥æé«˜å…³é”®è¯å‡ºç°é¢‘ç‡ï¼ˆTerm Frequencyï¼‰ã€‚

```
<p><font color="#ffffff">computer parts, hardware</font></p>
```

ä¹Ÿå¯ä»¥é€šè¿‡ CSS ä¼ªè£…ï¼š

```
<p class="class2"> ... </p>
.class2 { color: white; }
```

**æœç´¢å¼•æ“å¤„ç†**ï¼š

- è¯†åˆ«éš¾åº¦å¤§ï¼Œå°¤å…¶æ˜¯ CSS éšè—
- ä¸€æ—¦è¢«å‘ç°ï¼Œ**å°†è¢«ç§»å‡ºç´¢å¼•æˆ–é™æƒå¤„ç†**

3. Cloakingï¼ˆæ¬ºéª—æ€§å†…å®¹æŠ•æ”¾ï¼‰

æ ¹æ®è®¿é—®è€…èº«ä»½ï¼ˆUser-Agentï¼‰åˆ¤æ–­å¹¶è¿”å›**ä¸åŒå†…å®¹**ï¼š

- å¯¹ç”¨æˆ·æ˜¾ç¤ºæ­£å¸¸é¡µé¢
- å¯¹çˆ¬è™«è¿”å›é«˜å¯†åº¦å…³é”®è¯çš„â€œä¼˜åŒ–â€é¡µé¢

ä¹Ÿå¯èƒ½é€šè¿‡ JavaScript é‡å®šå‘æˆ– `noscript` æ ‡ç­¾æ¬ºéª—çˆ¬è™«ã€‚

> è¿™ç§è¡Œä¸ºä¸¥é‡è¿åæœç´¢å¼•æ“è§„åˆ™ã€‚

4. Sneaky JavaScriptï¼ˆåˆ©ç”¨ JavaScript æ¬ºéª—ï¼‰

é€šè¿‡ `<noscript>` æ ‡ç­¾å†…åµŒå…¥ä¼˜åŒ–å†…å®¹ï¼Œæˆ–åœ¨ JavaScript ä¸­è®¾ç½®é‡å®šå‘ï¼Œæ¬ºéª—æœç´¢å¼•æ“ã€‚

```
htmlå¤åˆ¶ç¼–è¾‘<noscript>
  <p>Optimized keywords for search engines</p>
</noscript>
```

- ç”¨æˆ·çœ‹ä¸åˆ° `<noscript>` å†…å®¹
- çˆ¬è™«å´å°†å…¶å½“ä½œç½‘é¡µä¸»å†…å®¹è¿›è¡Œç´¢å¼•

5. Exploiting PageRankï¼ˆæ“çºµé“¾æ¥ç»“æ„ï¼‰

##### ä¸¤ç§æ–¹å¼ï¼š

- **Link Farmsï¼ˆé“¾æ¥å†œåœºï¼‰**ï¼š
  - åˆ›å»ºå¤§é‡äº’ç›¸é“¾æ¥çš„ç½‘é¡µï¼Œæé«˜å½¼æ­¤çš„ PageRank
  - é«˜ PR é¡µé¢å†æ”¶å–è´¹ç”¨å‡ºå”®å¤–é“¾
- **Comment Spamï¼ˆè¯„è®ºåƒåœ¾ï¼‰**ï¼š
  - åœ¨å¼€æ”¾è¯„è®ºçš„é¡µé¢å¤§é‡å‘å¸ƒå¸¦æœ‰å¤–é“¾çš„è¯„è®º
  - åˆ©ç”¨ PageRank ä¼ é€’æƒé‡

**è§£å†³åŠæ³•**ï¼š

- ä½¿ç”¨ `rel="nofollow"` å±æ€§é˜»æ­¢ PR ä¼ é€’

6. Doorway Pagesï¼ˆè·³è½¬é¡µé¢ï¼‰

ä¸ºæ¯ä¸ªé«˜é¢‘æœç´¢è¯åˆ›å»ºä¸€ä¸ªç‹¬ç«‹é¡µé¢ï¼Œæ¯é¡µéƒ½é‡å®šå‘åˆ°åŒä¸€ç›®æ ‡ç½‘ç«™ã€‚

ç›®çš„ï¼š**ä¼˜åŒ–å•ä¸€å…³é”®è¯**ï¼Œæå‡ä¸»ç«™æµé‡ã€‚